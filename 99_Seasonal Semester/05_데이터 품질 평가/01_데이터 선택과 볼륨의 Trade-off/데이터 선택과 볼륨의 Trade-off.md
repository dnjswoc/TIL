# 2024년 12월 6일(금) 수업 내용 정리 - 데이터 품질 평가


## 데이터 선택과 볼륨의 정의, 데이터 선택과 볼륨간 Trade-off 사례

- 데이터 선택과 볼륨

  - 데이터 선택
  - LLM에서의 데이터 선택
  - 데이터 볼륨


- 데이터 선택과 볼륨 간 Trade-off 사례

  - 적절한 데이터 볼륨과 데이터 선택 사례

<hr>

### 01 데이터 선택과 볼륨

- 학습 목표

  - 데이터 선택과 데이터 볼륨이 LLM에 미치는 영향을 이해한다
  - 모델 개발 시 적합한 데이터 전략을 수립할 수 있다
  
<hr>

#### 데이터 선택

- 데이터 선택은 머신러닝에서 오래된 과제로, 주어진 raw data에서 특정 목표 함수에 대해 최적의 데이터셋을 설계하는 것을 목표로 함
- 특히 대규모 언어 모델(LLM, Large Language Model) 훈련에서 핵심적인 역할을 함

<hr>

#### LLM에서의 데이터 선택

- 대규모 언어 모델(LLM)은 방대한 데이터를 사용하기 때문에 효율적이고 효과적인 데이터 선택이 필수
- 대규모 언어 모델(LLM) 학습에서 데이터 선택 방식은 데이터 파이프라인의 각 단계와 학습 목적에 따라 다양하게 적용

  ![alt text](./images/image_00.png)

**각 학습 단계에 따라 데이터 선택 전략이 달라짐. 각 단계에서 어떤 데이터를 선택애햐 하는지 명확하게 이해해야 함**

1. Pretraining
2. Instruction Tuning
3. Alignment
4. Task-specific Fine-tuning

##### 1. Pretraining(사전학습) 단계에서의 데이터 선택

- Pretraining 단계는 언어 모델 학습의 초기 단계로, 범용성 있는 모델을 구축하기 위해 다양한 데이터 선택 방법을 사용.
- 이 단계에서는 방대한 양의 데이터에서 **고품질, 효율적인 데이터**를 선정하고, **불필요한 노이즈와 중복 데이터**를 제거하는 것이 핵심

  ![alt text](./images/image_01.png)

(1) Language Filtering

- Language Filtering의 주요 목적

  - 사전학습 데이터에서 특정 언어(자연어 또는 프로그래밍 언어)만 포함시키는 작업
  - 다국어 모델의 경우 언어 간 데이터 비율을 유지하거나, 특정 언어를 제외하기도 함

    ![alt text](./images/image_02.png)

    ① **문자 n-gram 기반 분류기** : 텍스트에서 문자 n-gram(예 : 2글자 또는 3글자 조합)을 사용하여 언어를 판별

      - langdetect : 간단하고 빠르게 언어를 감지
      - cld3 : Google에서 개발한 도구로 다국어 감지 성능 우수
      - FastText : Facebook에서 개발한 도구로, 많은 언어를 지원하며 속도와 정확도가 뛰어남

        ![alt text](./images/image_03.png)

    ② **비영어 데이터 제거** : 영어 데이터만 남기고 다른 언어 데이터를 모두 제거

      - C4 데이터셋에서는 langdetect 분류기를 사용해 영어가 아닌 페이지를 제거(99% 이상의 확률로 영어로 분류된 데이터만 유지)

        ![alt text](./images/image_04.png)

    ③ **다국어 데이터 필터링** : 여러 언어를 포함한 데이터셋을 다룰 때, 각 언어별 임계값을 설정해 데이터를 필터링

      - **ROOT 코퍼스**에서는 FastText를 사용해 언어별 신뢰도 점수를 계산하고, 기준 미달 데이터를 제거

        ![alt text](./images/image_05.png)

    ④ **URL 및 도메인 기반 필터링** : 특정 국가 도메인(.kr, .fr 등)이나 URL 패턴을 사용해 언어를 간접적으로 판별

      - **도메인 확장자(예 : .kr, .fr, .de)를 사용해 특정 국가와 관련된 데이터를 필터링**
      - 낮은 자원 언어(예 : 위구르어)의 경우 언어 분류기보다 URL 기반 필터링이 더 효과적
      - 분류기가 지원하지 않는 언어에도 적용 가능
      - 저자원 언어 데이터를 상대적으로 신뢰성 있게 수집 가능
      - URL 패턴만으로 언어를 완벽히 판별하기는 어렵다는 단점이 있음

    ⑤ **코드 데이터 필터링** : 코드 파일의 확장자(예 : .py, .java) 또는 특정 패턴을 사용해 코드 데이터를 필터링

      - GitHub 저장소의 스냅샷 데이터를 처리할 때 파일 확장자를 기반으로 프로그래밍 언어를 식별
      - 간단하고 빠르게 코드 데이터를 선별 가능
      - 코드와 자연어가 섞인 문서에서는 코드 부분을 정확히 분리하기 어려움


(2) Heuristic Approaches

  ① **항목 개수를 기준으로 필터링** : 데이터의 문장 수, 단어 수, 문자 수 등의 통계 기준으로 필터링

  - C4 데이터셋은 5문장 미만의 페이지를 제거
  - MassiveText는 50~100,000 단어 범위를 초과하는 문서를 제거
  
  ② **반복성 제거** : 반복적으로 등장하는 n-gram, 문장, 문단이 많은 데이터를 제거

  - MassiveText는 문서에서 반복된 문장이 30%를 넘으면 해당 문서를 제거
  
  ③ **특정 패턴 및 블랙리스트 기반 필터링** : 블랙리스트 단어나 패턴이 포함된 데이터를 제거

  - C4는 부적절한 단어 리스트에 포함된 단어를 가진 문서를 제거
  - HTML 태그, 광고 문구, 오류 메시지 등을 제거

<br>

- 단순한 휴리스틱은 품질을 직접 판단하지 못하며, 유용한 데이터까지 제거할 위험이 있음
- 새로운 휴리스틱의 효과를 검증하려면 많은 시간과 비용이 소요


(3) Data Quality : **고품질** 데이터를 선별해 모델 성능을 높이고, 불필요한 데이터를 제거

  ① **고품질의 참조 데이터셋과 비교** : 고품질 참조 데이터셋(위키피디아, 책, 뉴스기사 등)을 기준으로 유사성 점수를 계산해 데이터 선택

  - GPT-3는 Web Text와 유사한 데이터를 포함시키는 분류기를 사용
  - FastText, n-gram, 해시 기반 분류기로 대규모 데이터를 빠르게 처리

    - FastText : Facebook에서 개발한 언어 모델로, 텍스트를 빠르게 분류하고 유사성을 계산
    - N-gram : 텍스트를 일정 길이로 쪼개 참조 코퍼스와 유사도를 비교
    - Hash : 텍스트를 숫자로 변환해 비교를 간단하게 만듦
  
  ② **퍼플렉시티 기반 평가**

  - 참조 데이터셋(위키피디아, 학술논문 등)으로 학습된 언어 모델을 사용해 데이터의 퍼플렉시티 점수를 계산
  - 퍼플렉시티가 낮을수록 참조 도메인에 가까운 데이터로 간주
  - CCNet은 5-gram 언어 모델로 문단별 퍼플렉시티를 평가해 데이터를 필터링

    ![alt text](./images/image_06.png)


(4) 도메인 특화 필터링

- **Moore-Lewis 필터링** : 특정 분야(도메인)에 적합한 데이터를 고르는 방법, 어떤 데이터가 내가 원하는 분야(의학, 법률 등)에 더 잘 맞는지를 계산하는 방식

  - 의학, 법률, 기술 등 특화딘 주제에 대해 모델의 성능을 높이는 것이 목표
  - 두개의 모델을 사용, 특정 분야에서 학습된 도메인 모델과 일반적인 데이터에서 학습된 일반 모델을 가지고 새로운 데이터를 두 모델에 넣고, 각 모델이 이 데이터와 얼마나 잘 맞는다고 생각하는지 점수를 계산
  - 도메인 모델의 점수가 일반 모델보다 높다면 이 데이터는 도메인에 적합하다고 판단


