{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Objectives**\n",
    "\n",
    "1. ì‹¤ìŠµ ê°œìš”\n",
    "    - RAG (Retrieval-Augmented Generation)ì˜ ê°œë…ì„ ì´í•´í•˜ê³  í™œìš©ë²•ì„ í•™ìŠµ.\n",
    "    - Upstageì˜ Solar LLMì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ê³„ë³„ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•.\n",
    "    - RAG ì‹œìŠ¤í…œ ì„¤ê³„ì™€ êµ¬í˜„ì„ í†µí•´ ì–¸ì–´ ëª¨ë¸ì˜ ì‘ìš© ëŠ¥ë ¥ ê°•í™”.\n",
    "\n",
    "2. ì‹¤ìŠµ ì§„í–‰ ëª©ì  ë° ë°°ê²½\n",
    "    - ëª©ì : RAG ê°œë…ê³¼ Solar LLMì„ í™œìš©í•˜ì—¬ ì •ë³´ ê²€ìƒ‰ê³¼ ì–¸ì–´ ìƒì„±ì„ ê²°í•©í•œ ì‹¤ë¬´í˜• ì‹œìŠ¤í…œ êµ¬í˜„ ì—­ëŸ‰ì„ ê°•í™”í•©ë‹ˆë‹¤.\n",
    "    - ë°°ê²½: RAGëŠ” ì–¸ì–´ ëª¨ë¸ê³¼ ì •ë³´ ê²€ìƒ‰ ê¸°ìˆ ì˜ ìœµí•©ìœ¼ë¡œ ë³µì¡í•œ ë¬¸ì œ í•´ê²°ì— í™œìš©ë˜ë©°, ì‹¤ì œ ì‹¤ë¬´ì—ì„œ ë§ì´ í™œìš©ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´, ì‹¤ì œ ì‹¤ë¬´ì—ì„œ ì ìš©í•  ìˆ˜ ìˆëŠ” ì—­ëŸ‰ì„ ê°•í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "3. ì‹¤ìŠµ ìˆ˜í–‰ìœ¼ë¡œ ì–»ì–´ê°ˆ ìˆ˜ ìˆëŠ” ì—­ëŸ‰\n",
    "    - RAG ê°œë…ì„ ì´í•´í•˜ê³  ì´ë¥¼ ì‹¤ì œë¡œ êµ¬í˜„í•˜ëŠ” ëŠ¥ë ¥.\n",
    "    - LangChainì„ í™œìš©í•œ LLM Chain ì„¤ê³„ ë° ì±—ë´‡ ê°œë°œ ê¸°ìˆ .\n",
    "\n",
    "4. ì‹¤ìŠµ í•µì‹¬ ë‚´ìš©\n",
    "    - LLM Chain êµ¬í˜„: LangChainì„ í™œìš©í•˜ì—¬ ê°„ë‹¨í•œ ChatBotì„ ì„¤ê³„í•˜ê³  êµ¬í˜„.\n",
    "    - RAG ì‹œìŠ¤í…œ êµ¬ì¶•: RAGì˜ ê°œë…ì„ ì´í•´í•˜ê³  ì´ë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ê³„ ë° êµ¬í˜„.\n",
    "    - LLM API í™œìš©: Upstageì˜ Solar LLM APIë¥¼ í†µí•´ LLMì„ í˜¸ì¶œí•˜ê³  ì‹œìŠ¤í…œì— í†µí•©.\n",
    "    - ì‹¤ë¬´ í”„ë¡œì íŠ¸ ê¸°ë°˜ í•™ìŠµ: ì‹¤ì§ˆì ì¸ í”„ë¡œì íŠ¸ ì„¤ê³„ë¥¼ í†µí•´ ì‹¤ë¬´ ì ìš© ê°€ëŠ¥ì„± í–¥ìƒ.   \n",
    "\n",
    "â­ ì´ë²ˆì—ëŠ” ì—¬ëŸ¬ë¶„ë“¤ë§Œì˜ ì„œë¹„ìŠ¤ ê¸°íšì•ˆì— ë”°ë¼ ë°ì´í„°ë¥¼ ë°”ê¾¸ì–´ ì§„í–‰í•´ë³´ì„¸ìš”. ë³¸ ì‹¤ìŠµì€ í”„ë¡œì íŠ¸ ì˜ˆì‹œë¡œ ì‘ì„±ëœ ì œí’ˆ ë§¤ë‰´ì–¼ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prerequisties**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "langchain == 0.3.12\n",
    "langchain-chroma == 0.1.4\n",
    "langchain-core == 0.3.25\n",
    "langchain-openai == 0.2.12\n",
    "langchain-text-splitters == 0.3.3\n",
    "langchain-upstage == 0.4.0\n",
    "getpass4 == 0.0.14.1\n",
    "openai == 1.57.4\n",
    "ragas == 0.2.8\n",
    "langchain_community == 0.3.12\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ì•½ 1ë¶„ ì†Œìš”\n",
    "!pip install -qU openai langchain langchain-upstage langchain-chroma getpass4 ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAGAS callback í•¨ìˆ˜ ìˆ˜ì •\n",
    "- ê²½ë¡œ: `/usr/local/lib/python3.10/dist-packages/ragas/callbacks.py`\n",
    "- ragasë¥¼ ë‹¤ìš´ë¡œë“œ ë°›ê³ , ìœ„ íŒŒì¼ ê²½ë¡œì˜ ê°’ì„ ì•„ë˜ ì½”ë“œë¡œ ë°”ê¿”ë¼ì›ë‹ˆë‹¤.\n",
    "- ì‹¤í–‰í•œ í›„ ì„¸ì…˜ì„ ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤. ì„¸ì…˜ì„ ë‹¤ì‹œ ì‹œì‘í•œ í›„ ì•„ë˜ set API keyë¶€í„° ì‹¤í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import typing as t\n",
    "import uuid\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    BaseCallbackHandler,\n",
    "    CallbackManager,\n",
    "    CallbackManagerForChainGroup,\n",
    "    CallbackManagerForChainRun,\n",
    "    Callbacks,\n",
    ")\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "def new_group(\n",
    "    name: str,\n",
    "    inputs: t.Dict,\n",
    "    callbacks: Callbacks,\n",
    "    tags: t.Optional[t.List[str]] = None,\n",
    "    metadata: t.Optional[t.Dict[str, t.Any]] = None,\n",
    ") -> t.Tuple[CallbackManagerForChainRun, CallbackManagerForChainGroup]:\n",
    "    tags = tags or []\n",
    "    metadata = metadata or {}\n",
    "\n",
    "    # start evaluation chain\n",
    "    if isinstance(callbacks, list):\n",
    "        cm = CallbackManager.configure(inheritable_callbacks=callbacks)\n",
    "    else:\n",
    "        cm = t.cast(CallbackManager, callbacks)\n",
    "    cm.tags = tags\n",
    "    cm.metadata = metadata\n",
    "    rm = cm.on_chain_start({\"name\": name}, inputs)\n",
    "    child_cm = rm.get_child()\n",
    "    group_cm = CallbackManagerForChainGroup(\n",
    "        child_cm.handlers,\n",
    "        child_cm.inheritable_handlers,\n",
    "        child_cm.parent_run_id,\n",
    "        parent_run_manager=rm,\n",
    "        tags=child_cm.tags,\n",
    "        inheritable_tags=child_cm.inheritable_tags,\n",
    "        metadata=child_cm.metadata,\n",
    "        inheritable_metadata=child_cm.inheritable_metadata,\n",
    "    )\n",
    "\n",
    "    return rm, group_cm\n",
    "\n",
    "\n",
    "class ChainType(Enum):\n",
    "    EVALUATION = \"evaluation\"\n",
    "    METRIC = \"metric\"\n",
    "    ROW = \"row\"\n",
    "    RAGAS_PROMPT = \"ragas_prompt\"\n",
    "\n",
    "\n",
    "class ChainRun(BaseModel):\n",
    "    run_id: str\n",
    "    parent_run_id: t.Optional[str]\n",
    "    name: str\n",
    "    inputs: t.Dict[str, t.Any]\n",
    "    metadata: t.Dict[str, t.Any]\n",
    "    outputs: t.Dict[str, t.Any] = Field(default_factory=dict)\n",
    "    children: t.List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class ChainRunEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, uuid.UUID):\n",
    "            return str(o)\n",
    "        if isinstance(o, ChainType):\n",
    "            return o.value\n",
    "        # if isinstance(o, EvaluationResult):\n",
    "        #     return \"\"\n",
    "        return json.JSONEncoder.default(self, o)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RagasTracer(BaseCallbackHandler):\n",
    "    traces: t.Dict[str, ChainRun] = field(default_factory=dict)\n",
    "\n",
    "    def on_chain_start(\n",
    "        self,\n",
    "        serialized: t.Dict[str, t.Any],\n",
    "        inputs: t.Dict[str, t.Any],\n",
    "        *,\n",
    "        run_id: uuid.UUID,\n",
    "        parent_run_id: t.Optional[uuid.UUID] = None,\n",
    "        tags: t.Optional[t.List[str]] = None,\n",
    "        metadata: t.Optional[t.Dict[str, t.Any]] = None,\n",
    "        **kwargs: t.Any,\n",
    "    ) -> t.Any:\n",
    "        self.traces[str(run_id)] = ChainRun(\n",
    "            run_id=str(run_id),\n",
    "            parent_run_id=str(parent_run_id) if parent_run_id else None,\n",
    "            name=serialized[\"name\"],\n",
    "            inputs=inputs,\n",
    "            metadata=metadata or {},\n",
    "            children=[],\n",
    "        )\n",
    "\n",
    "        if parent_run_id and str(parent_run_id) in self.traces:\n",
    "            self.traces[str(parent_run_id)].children.append(str(run_id))\n",
    "\n",
    "    def on_chain_end(\n",
    "        self,\n",
    "        outputs: t.Dict[str, t.Any],\n",
    "        *,\n",
    "        run_id: uuid.UUID,\n",
    "        **kwargs: t.Any,\n",
    "    ) -> t.Any:\n",
    "        self.traces[str(run_id)].outputs = outputs\n",
    "\n",
    "    def to_jsons(self) -> str:\n",
    "        return json.dumps(\n",
    "            [t.model_dump() for t in self.traces.values()],\n",
    "            cls=ChainRunEncoder,\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MetricTrace(dict):\n",
    "    scores: t.Dict[str, float] = field(default_factory=dict)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.scores.__repr__()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "def parse_run_traces(\n",
    "    traces: t.Dict[str, ChainRun],\n",
    "    parent_run_id: t.Optional[str] = None,\n",
    ") -> t.List[t.Dict[str, t.Any]]:\n",
    "\n",
    "    root_traces = [\n",
    "        chain_trace\n",
    "        for chain_trace in traces.values()\n",
    "        if chain_trace.parent_run_id == parent_run_id\n",
    "    ]\n",
    "\n",
    "    if len(root_traces) > 1:\n",
    "        raise ValueError(\n",
    "            \"Multiple root traces found! This is a bug on our end, please file an issue and we will fix it ASAP :)\"\n",
    "        )\n",
    "    root_trace = root_traces[0]\n",
    "\n",
    "    # get all the row traces\n",
    "    parased_traces = []\n",
    "    for row_uuid in root_trace.children:\n",
    "        row_trace = traces[row_uuid]\n",
    "        metric_traces = MetricTrace()\n",
    "        for metric_uuid in row_trace.children:\n",
    "            metric_trace = traces[metric_uuid]\n",
    "            metric_traces.scores[metric_trace.name] = metric_trace.outputs.get(\n",
    "                \"output\", {}\n",
    "            )\n",
    "            # get all the prompt IO from the metric trace\n",
    "            prompt_traces = {}\n",
    "            for i, prompt_uuid in enumerate(metric_trace.children):\n",
    "                prompt_trace = traces[prompt_uuid]\n",
    "                output = prompt_trace.outputs.get(\"output\", {})\n",
    "                output = output[0] if isinstance(output, list) else output\n",
    "                prompt_traces[f\"{prompt_trace.name}\"] = {\n",
    "                    \"input\": prompt_trace.inputs.get(\"data\", {}),\n",
    "                    \"output\": output,\n",
    "                }\n",
    "            metric_traces[f\"{metric_trace.name}\"] = prompt_traces\n",
    "        parased_traces.append(metric_traces)\n",
    "\n",
    "    return parased_traces\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Get the Upstage API key using getpass\n",
    "try:\n",
    "    if \"UPSTAGE_API_KEY\" not in os.environ or not os.environ[\"UPSTAGE_API_KEY\"]:\n",
    "        os.environ[\"UPSTAGE_API_KEY\"] = getpass.getpass(\"Enter your Upstage API key: \")\n",
    "\n",
    "    print(\"API key has been set successfully.\")\n",
    "\n",
    "except:\n",
    "    print(\"Something wrong with your API KEY. Check your API Console again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise Overview**\n",
    "## **ì‹¤ìŠµ ëª©ì°¨**\n",
    "-  **RAG íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ê¸°ë²• ì†Œê°œ**:  RAG ê°œë…ì„ ì´í•´í•˜ê³  ë‹¨ê³„ë³„ë¡œ RAGë¥¼ ì§ì ‘ êµ¬í˜„í•´ë³´ì<br>\n",
    "    - 1-1 What is RAG?\n",
    "    - 1-2 ë‹¨ê³„ë³„ RAG êµ¬í˜„í•´ë³´ê¸°\n",
    "      - 1-2-1 ë¬¸ì„œ ì „ì²˜ë¦¬ (Document Preprocessing) <br>\n",
    "        (1) Loading Documents by `Upstage Document Parse API` <br>\n",
    "        (2) Text Splitting <br>\n",
    "        (3) Text Embedding  by `Upstage Solar Embedding API` <br>\n",
    "        (4) Vectorstore <br>\n",
    "      - 1-2-2 RAG ì‹¤í–‰í•˜ê¸°  <br>\n",
    "       (1) Retrieving Related Chunks from the Vectorstore <br>\n",
    "       (2) Creating a Prompt with Retrieved Results <br>\n",
    "       (3) Implementing an LLM Chain <br>\n",
    "       (4) Executing the LLM Chain and Generating the Response\n",
    "    - 1-3 Webì—ì„œ ë°ì´í„° ê°€ì ¸ì™€ì„œ RAG êµ¬ì¶• ì—°ìŠµí•´ë³´ê¸°\n",
    "    - 1-4 Open Source Tool(RAGAS)ë¥¼ ì‚¬ìš©í•œ RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1 What is RAG\n",
    "\n",
    "![Overview](https://drive.google.com/uc?id=18nYSFhTdlhmlXGEaiENEzfJL9N6v23XI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“˜  RAG ë€?\n",
    "\n",
    "- ì •ì˜ :  RAG(Retrieval-Augmented Generation)ëŠ” **ê²€ìƒ‰ ì—”ì§„**ê³¼ **LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)**ì„ ê²°í•©í•œ ê¸°ìˆ ë¡œ, ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ëŒ€í•´ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ê²€ìƒ‰ëœ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ LLMì´ ë” ì •í™•í•˜ê³  ë§¥ë½ì— ë§ëŠ” ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.\n",
    "- ë°©ì‹\n",
    "  - ì´ ê¸°ìˆ ì€ LLMì´ ë³´ìœ í•˜ì§€ ì•ŠëŠ” ì •ë³´ë¥¼ ë³´ì¶©í•˜ê¸° ìœ„í•´ ì™¸ë¶€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "  - ë” êµ¬ì²´ì ìœ¼ë¡œëŠ” ì™¸ë¶€ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ í†µí•´ì„œ ì´ë¥¼ ì¡°ê°(chunks)ìœ¼ë¡œ ë¶„ë¦¬í•œ í›„, ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì¡°ê°ì„ ê°€ì ¸ì™€ LLM í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì™¸ë¶€ ì •ë³´ë¥¼ LLMì—ê²Œ ì œê³µí•œ í›„, LLMì´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "- ê¸°ëŒ€íš¨ê³¼ : LLMì´ ìì²´ì ìœ¼ë¡œ ë³´ìœ í•˜ì§€ ì•ŠëŠ” ì •ë³´ë¥¼ ì™¸ë¶€ ë¬¸ì„œë¥¼ í†µí•´ ë³´ì¶©í•´ì£¼ê¸°ì—, LLMì˜ í•œê³„ì¸ Hallucinationê³¼ Knowledge Cutoff ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“˜ ê° ë‹¨ê³„ë³„ ì„¤ëª…\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1hwyxaRToKo7JbaupBctR0KIKOgz3jOsm\" alt=\"Definition of DP\" width=\"800\" />\n",
    "\n",
    "1. **ë¬¸ì„œ ì „ì²˜ë¦¬ (Document Preprocessing)**:\n",
    "   1. **ë¬¸ì„œ ë¡œë“œ (Loading Documents)**: `Upstage Document Parse API`ë¥¼ ì‚¬ìš©í•´ ì™¸ë¶€ PDF ë¬¸ì„œë¥¼ HTML í…ìŠ¤íŠ¸ ë°ì´í„° í˜•íƒœë¡œ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "   2. **í…ìŠ¤íŠ¸ ë¶„í•  (Text Splitting)**: ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°(chunks)ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ê²€ìƒ‰ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ë³´ë‹¤ êµ¬ì²´ì ì´ê³  ì •í™•í•˜ê²Œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "   3.  **ì„ë² ë”© (Embedding)**: `Upstage Solar Embedding API`ë¥¼ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ ì‘ì—…ì€ ë¬¸ì„œì˜ ì˜ë¯¸/ë¬¸ë§¥ì„ ë²¡í„° ê³µê°„ì— ë§¤í•‘í•˜ì—¬ ëª¨ë¸ì´ ì´ë¥¼ ê²€ìƒ‰ ë° ë¶„ì„ì— í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
    "   4. **ë²¡í„°ìŠ¤í† ì–´ (Vectorstore)**: ì„ë² ë”©ëœ ë²¡í„°ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ë¡œ, RAG ì‹œìŠ¤í…œ ë‚´ì—ì„œ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ê³µê°„ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "2. **RAG ì‹¤í–‰ (Implementing RAG)**:\n",
    "   5. **ê´€ë ¨ ì¡°ê° ê²€ìƒ‰ (Retrieving Related Chunks)**: ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì¡°ê°ì„ ê²€ìƒ‰í•´ ê°€ì ¸ì˜µë‹ˆë‹¤. ì´ë¥¼ í†µí•´ LLMì´ ì‘ë‹µì„ ìƒì„±í•  ë•Œ í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "   6. **í”„ë¡¬í”„íŠ¸ ìƒì„± (Creating a Prompt with Retrieved Results)**: ê²€ìƒ‰ëœ ì¡°ê°ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ì‹œì¼œ LLMì´ ë¬¸ë§¥ì„ ì´í•´í•˜ê³  ì‘ë‹µí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
    "   7. **LLM ì²´ì¸ êµ¬í˜„ (Implementing an LLM Chain)**: ìœ„ì˜ ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLM ì²´ì¸ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "   8. **LLM ì²´ì¸ ì‹¤í–‰ ë° ì‘ë‹µ ì¶œë ¥ (Executing the LLM Chain and Generating the Response)**: LLM ì²´ì¸ì„ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2-1 ë¬¸ì„œ ì „ì²˜ë¦¬ (Docuemnt Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Loading Documents by `Upstage Document Parse API`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Upstage Document Parse API`ë¥¼ ì‚¬ìš©í•´ ì™¸ë¶€ PDF ë¬¸ì„œë¥¼ HTML í…ìŠ¤íŠ¸ ë°ì´í„° í˜•íƒœë¡œ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "\n",
    "- Upstage Document Parse extracts layouts, tables, and figures from any document\n",
    "- LangChain provides powerful tools for text splitting and vectorization\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1vw9GqpzZDrAAfNdUL_ko7-VvJlRZSDou\" alt=\"Definition of DP\" width=\"600\" />\n",
    "\n",
    "**ì •ì˜**\n",
    "- `Upstage Document Parse`ëŠ” ë³µì¡í•œ ë¬¸ì„œë¥¼ LLMì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë„êµ¬\n",
    "- LLMì€ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ(QA), ë¬¸ì„œ ìš”ì•½ ë“±ì˜ ìì—°ì–´ taskì—ëŠ” ìœ ìš©í•˜ì§€ë§Œ, ë¬¸ì„œ íŒŒì¼ì„ ë°”ë¡œ ì½ê³  ì²˜ë¦¬í•  ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤.\n",
    "\n",
    "**ì‘ë™ ë°©ì‹**\n",
    "- ë¬¸ì„œ íŒŒì¼ì„ HTMLì´ë‚˜ Markdownê³¼ ê°™ì€ LLMì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "- ë¬¸ì„œì˜ ë ˆì´ì•„ì›ƒ, í‘œ í˜•ì‹, ì´ë¯¸ì§€ ë“± ë‹¤ì–‘í•œ ìš”ì†Œë“¤ì„ ëª¨ë‘ ë°˜ì˜í•˜ì—¬ ë³€í™˜ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1yAbnaXDI6t_wTTV1i03GDqqoanN24bDv\" alt=\"DP\" width=\"600\" />\n",
    "\n",
    "**ì„±ëŠ¥**\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1PfgSNDsWr8RgBua7fO5eIDsq43B_W2QH\" alt=\"Definition of DP\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ì‹¤ìŠµì„ ì§„í–‰í•  íŒŒì¼ì€ Galaxy A12 ë§¤ë‰´ì–¼ ë¬¸ì„œì…ë‹ˆë‹¤.\n",
    "!wget -O galaxy_A12.pdf https://files.customersaas.com/files/AOVV-atOIDUSB5NELiKGDtk8.pdf\n",
    "\n",
    "# ì‹¤ì œë¡œ ì§„í–‰í•  íŒŒì¼ì€ ì•„ë˜ ë°©ì‹ìœ¼ë¡œ ë¡œë“œí•˜ì—¬ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ğŸ“ ë‹¤ìš´ë¡œë“œ ëœ íŒŒì¼ í™•ì¸í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "1. Colab í™”ë©´ ì™¼ìª½ íŒŒì¼ ì´ëª¨ì§€ë¥¼ í´ë¦­í•´ë³´ë©´ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ galaxy_A12.pdfê°€ ì˜ ë‹¤ìš´ë¡œë“œ ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "2. ë§Œì•½ ìœ„ ê³¼ì •ì„ ì‹¤íŒ¨í•˜ì…¨ê±°ë‚˜ ë‹¤ë¥¸ PDF ë¬¸ì„œë¥¼ ì§ì ‘ ì—…ë¡œë“œ í›„ í™•ì¸í•´ë³´ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´, ìƒë‹¨ì˜ ì²« ë²ˆì§¸ ë²„íŠ¼ í´ë¦­ í›„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "- [Galaxy_A12 ë©”ë‰´ì–¼ PDF ë§í¬](https://files.customersaas.com/files/AOVV-atOIDUSB5NELiKGDtk8.pdf) ì—ì„œ ë¡œì»¬ì— ì§ì ‘ ë‹¤ìš´ë¡œë“œ ë°›ìœ¼ì‹  í›„ ì—…ë¡œë“œ\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1p3PoZW4r_v_dpNsSypZYJU-Lp0dlaJdk\" alt=\"Colab File Emoji\" width=\"400\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Document Parseë¡œ ë‹¤ìš´ë¡œë“œ ëœ ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "%%time\n",
    "\n",
    "from langchain_upstage import UpstageDocumentParseLoader\n",
    "\n",
    "layzer = UpstageDocumentParseLoader(\n",
    "    \"galaxy_A12.pdf\", # ë¶ˆëŸ¬ì˜¬ íŒŒì¼\n",
    "    output_format='html',  # ê²°ê³¼ë¬¼ í˜•íƒœ : HTML\n",
    "    coordinates= False) # ì´ë¯¸ì§€ OCR ì¢Œí‘œê³„ ê°€ì§€ê³  ì˜¤ì§€ ì•Šê¸°\n",
    "\n",
    "docs = layzer.load()\n",
    "\n",
    "# layzer.lazy_load()\n",
    "# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ì„œ, lazy_load() ë¡œ í˜ì´ì§€ë³„ë¡œ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# HTML í˜•íƒœ í™•ì¸í•´ë³´ê¸°\n",
    "for doc in docs:\n",
    "    print(doc.page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ë¶ˆëŸ¬ì˜¨ HTML í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ í˜•íƒœë¡œ í™•ì¸í•´ë³´ê¸°\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(docs[0].page_content[:2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Text Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**ğŸ“˜Text Splittingì„ í•˜ëŠ” ì´ìœ **\n",
    "- ë§Œì•½, ë¬¸ì„œê°€ ê¸¸ì§€ ì•Šê³  ì§§ë‹¤ë©´ ê·¸ëŒ€ë¡œ ë„£ì–´ì£¼ë„ ë¬´ë°©í•¨.\n",
    "- í•˜ì§€ë§Œ, ë¬¸ì„œì˜ ì‚¬ì´ì¦ˆê°€ í° ê²½ìš°\n",
    "  - LLMì—ê²Œ ì…ë ¥í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ í† í° ìˆ˜ë¥¼ ì´ˆê³¼\n",
    "  - ì¿¼ë¦¬ì™€ ì—°ê´€ë˜ì§€ ì•Šì€ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ì´ í¬í•¨ë˜ì–´ í—·ê°ˆë¦¬ê²Œ í•¨ìœ¼ë¡œì¨ ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¤ì§€ ì•ŠìŒ.\n",
    "- ë”°ë¼ì„œ  ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°(chunks)ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ê²€ìƒ‰ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ë³´ë‹¤ êµ¬ì²´ì ì´ê³  ì •í™•í•˜ê²Œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "<b>ğŸ“˜RecursiveCharacterTextSplitter </b>\n",
    "- ê°€ì¥ ëŒ€í‘œì ìœ¼ë¡œ ë§ì´ í™œìš©ë˜ëŠ” Text Splitter\n",
    "- **ë°©ì‹**\n",
    "  - `RecursiveCharacterTextSplitter` : ê¸€ììˆ˜ê°€ `chunk size`ë¥¼ ë„˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ì•„ë˜ì™€ ê°™ì´ textë¥¼ ë¶„ë¦¬í•¨.\n",
    "  -  `(['\\n\\n', '\\n', ' ', ''])` : ë¬¸ë‹¨, ë¬¸ì¥, ë‹¨ì–´ ìˆœì„œë¡œ ë¬¸ì„œë¥¼ ë¶„ë¦¬í•˜ë‹¤ê°€ chunk ë‚´ ê¸€ììˆ˜ê°€ `chunk size` ë³´ë‹¤ ì‘ì•„ì§€ë©´ ë¶„ë¦¬í•¨.\n",
    "- **íŒŒë¼ë¯¸í„°**\n",
    "  - `chunk_size` : í•˜ë‚˜ì˜ chunk ë‹¹ ìµœëŒ€ ê¸€ì ìˆ˜\n",
    "  - `chunk_overlap` : ê° chunk ë§ˆë‹¤ ê²¹ì¹˜ëŠ” ê¸€ì ìˆ˜\n",
    "    - chunk ë§ˆë‹¤ ì¼ë¶€ ë‚´ìš©ì„ ê²¹ì³ì„œ ë¶„í• ëœ chuck ë§ˆë‹¤ ë§¥ë½ì´ ì´ì–´ì§ˆìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# 2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(\"Splits:\", len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# splitëœ ê²°ê³¼ í™•ì¸ : chunk size 1000ë¡œ ëŒ€ë‹¤ìˆ˜ê°€ ë¶„ë¦¬ë¨.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "split_lengths = [len(split.page_content) for split in splits]\n",
    "\n",
    "# Create a bar graph\n",
    "plt.bar(range(len(split_lengths)), split_lengths)\n",
    "plt.title(\"RecursiveCharacterTextSplitter\")\n",
    "plt.xlabel(\"Split Index\")\n",
    "plt.ylabel(\"Split Content Length\")\n",
    "plt.xticks(range(len(split_lengths)), [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1dcmr0Q572fBwY-o4GN5mhCCise8WdN7C\" alt=\"solar embedding\" width=\"600\" />\n",
    "\n",
    "**Embedding ì •ì˜**\n",
    "- Embeddingì€ **ìì—°ì–´(í…ìŠ¤íŠ¸) ë°ì´í„°**ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” **ìˆ«ì** í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒ.\n",
    "- ì‚¬ëŒì´ ì´í•´í•˜ëŠ” í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹(=Vector)ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°ì´í„°ë¥¼ LLM ë“± ëª¨ë¸ì— ì‰½ê²Œ ì „ë‹¬í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
    "\n",
    "**`Upstage Solar Embedding API`**\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=18tPtT8_6M9ilT7q5FkBx9DGYMHj8yRIl\" alt=\"solar embedding\" width=\"600\" />\n",
    "\n",
    "- ì„±ëŠ¥ : í•œêµ­ì–´/ì˜ì–´/ì¼ë³¸ì–´ ëª¨ë‘ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚¨\n",
    "\n",
    "**RAGì—ì„œ Embeddingì€ VectorStore ì—ì„œ ëª¨ë¸ì„ ì§€ì •í•˜ì—¬ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) ë²¡í„°ìŠ¤í† ì–´ (VectorStore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ë²¡í„°**ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ë¡œ, RAG ì‹œìŠ¤í…œ ë‚´ì—ì„œ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ê³µê°„ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "- VectorStoreë§ˆë‹¤ ì§€ì›í•˜ëŠ” ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì´ ë‹¬ë¼ì§.\n",
    "- Vectorstore ìœ í˜•\n",
    "  -  ë¡œì»¬ : ë‚´ ì»´í“¨í„°ì— ì €ì¥\n",
    "  -  Cloud : ê° DB íšŒì‚¬ ì„œë²„ì— ì €ì¥\n",
    "- Langchainì´ ì œê³µí•˜ëŠ” ë” ë‹¤ì–‘í•œ VectorStore : [ë§í¬](https://python.langchain.com/v0.2/docs/integrations/vectorstores/)\n",
    "\n",
    "**ChromaDB**\n",
    "- Cloud ê¸°ë°˜ ì˜¤í”ˆì†ŒìŠ¤ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤\n",
    "- ì„ë² ë”© Vector ì €ì¥ ë° ê²€ìƒ‰ ê¸°ëŠ¥\n",
    "- ë¬¸ì„œ ì¶”ê°€ ë° ì—…ë°ì´íŠ¸, ì‚­ì œ\n",
    "- ë‹¤ì–‘í•œ ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ\n",
    "- Langchainê³¼ í†µí•©ë˜ì–´ ì œê³µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# 3. Embed & indexing\n",
    "vectorstore = Chroma.from_documents(\n",
    "     documents= splits, embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì°¸ê³ **\n",
    "- [Chroma LangChain Documentation](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)\n",
    "- [Chroma Official Documentation](https://docs.trychroma.com/getting-started)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###(Recap.)  OVERVIEW OF RAG\n",
    "\n",
    "- ì™¸ë¶€ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì„œ ì „ì²˜ë¦¬ê¹Œì§€ ì™„ë£Œ\n",
    "- ì´ì œ ê° chunk ë‹¨ìœ„ë¡œ ë¶„ë¦¬ëœ ë¬¸ì„œ ì¤‘ \"ì‚¬ìš©ìì˜ ì§ˆë¬¸(ì¿¼ë¦¬)\"ì™€ ê´€ë ¨í•œ chunkë¥¼ **ê²€ìƒ‰**í•´ë³´ì !\n",
    "\n",
    "![Overview](https://drive.google.com/uc?id=18nYSFhTdlhmlXGEaiENEzfJL9N6v23XI)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2-2 RAG ì‹¤í–‰í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) Retrieving Related Chunks\n",
    "\n",
    "- ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸(ì¿¼ë¦¬)ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. LLMì´ ì‘ë‹µì„ ìƒì„±í•  ë•Œ í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "- Retriever ìœ í˜•\n",
    "  - **Sparse Retriever**: í‚¤ì›Œë“œ ê²€ìƒ‰\n",
    "    - ì¿¼ë¦¬ë¥¼ í‚¤ì›Œë“œ ë²¡í„°ë¡œ ì „í™˜í•˜ì—¬ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "    - íŠ¹ì • ë„ë©”ì¸ ì§€ì‹(e.g. ì˜í•™, ë²•ë¥ ) ë“±ì„ ê²€ìƒ‰í•  ë•Œ ìš©ì´í•©ë‹ˆë‹¤.\n",
    "    - ë‹¨ì–´ë“¤ì´ ì§ì ‘ì ìœ¼ë¡œ ìœ ì‚¬í•œ ìƒí™©ì— ìš©ì´í•©ë‹ˆë‹¤.\n",
    "    - ëŒ€í‘œ ì•Œê³ ë¦¬ì¦˜ : TF-IDF, BM25\n",
    "  - **Dense Retriever**: ì˜ë¯¸ ê²€ìƒ‰\n",
    "    - ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ì„ë² ë”©í•˜ì—¬ ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ chunkë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "    - ë³µì¡í•œ ìì—°ì–´ ì§ˆë¬¸ì— ëŒ€í•œ ê²€ìƒ‰ ì‹œ ìš©ì´í•©ë‹ˆë‹¤.\n",
    "    - ë¬¸ë§¥ì  ìœ ì‚¬ë„ê°€ ì¤‘ìš”í•œ ìƒí™©ì— ìš©ì´í•©ë‹ˆë‹¤.\n",
    "    - ëŒ€í‘œ ì•Œê³ ë¦¬ì¦˜ : FAISS\n",
    "\n",
    "**ì°¸ê³ ** <br>\n",
    "- [Lanchain - Retriever](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/)  documentationì„ í†µí•´ ë” ìì„¸í•œ ë‚´ìš© ì°¸ê³  ë°”ëë‹ˆë‹¤\n",
    "- [MMR ì•Œê³ ë¦¬ì¦˜](https://wikidocs.net/231585) ì— ëŒ€í•œ ì„¤ëª…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ì‚¬ìš©ìì˜ ì§ˆë¬¸, ì¿¼ë¦¬\n",
    "query = \"Please tell me about galaxy a12\"\n",
    "\n",
    "# Dense Retriever ìƒì„±\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type= 'mmr', # default : similarity(ìœ ì‚¬ë„) / mmr ì•Œê³ ë¦¬ì¦˜\n",
    "    search_kwargs={\"k\": 3} # ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ chunkë¥¼ 3ê°œ ê²€ìƒ‰í•˜ê¸° (default : 4)\n",
    ")\n",
    "\n",
    "result_docs = retriever.invoke(query) # ì¿¼ë¦¬ í˜¸ì¶œí•˜ì—¬ retrieverë¡œ ê²€ìƒ‰\n",
    "\n",
    "print(len(result_docs))\n",
    "print(result_docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) Creating a Prompt with Retrieved Results\n",
    "- ê²€ìƒ‰ëœ ë¬¸ì„œ chunkë“¤ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ì‹œì¼œ LLMì´ ë¬¸ë§¥ì„ ì´í•´í•˜ê³  ì‘ë‹µí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# contextì— ê²€ìƒ‰ëœ chunkë“¤ì„ ë„£ì–´ì¤ë‹ˆë‹¤.\n",
    "# (ì§€ë‚œ ì‹œê°„ì— ë°°ì› ë˜) LLM Chain êµ¬ì„±í•˜ëŠ” ë²•.\n",
    "# 1. llm ì •ì˜, 2. prompt ì •ì˜, 3. chain ì •ì˜, 4. chain í˜¸ì¶œ\n",
    "\n",
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are an assistant for question-answering tasks.\n",
    "            Use the following pieces of retrieved context to answer the question considering the history of the conversation.\n",
    "            If you don't know the answer, just say that you don't know.\n",
    "            ---\n",
    "            CONTEXT:\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) Implementing an LLM Chain\n",
    "- ìœ„ì˜ ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLM ì²´ì¸ì„ ì •ì˜í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (8) Executing the LLM Chain and Generating the Response\n",
    "-  LLM ì²´ì¸ì„ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query = \"Please tell me about galaxy a12\"\n",
    "\n",
    "response = chain.invoke({\"context\": result_docs, \"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "response  # RAG + LLM -> naturalí•œ resopnse!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3 Webì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì™€ì„œ RAG êµ¬ì¶• ì—°ìŠµí•´ë³´ê¸°\n",
    "\n",
    "- langchain_communityì— ìˆëŠ” WebBaseLoaderë¥¼ ì´ìš©í•˜ì—¬ wikipediaì˜ ë‚´ìš©ì„ ë¶ˆëŸ¬ì™€ì„œ RAGë¥¼ êµ¬ì¶•í•˜ëŠ” ì˜ˆì œë¥¼ ì—°ìŠµí•´ë´…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# 1. ì›¹ í˜ì´ì§€ ë¡œë“œ\n",
    "loader = WebBaseLoader(\"https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%8D%AC\")\n",
    "data = loader.load()\n",
    "\n",
    "# 2. í…ìŠ¤íŠ¸ ë¶„í• \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "# 3. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "vectorstore = Chroma.from_documents(\n",
    "     documents= splits, embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    ")\n",
    "\n",
    "# 4. Dense Retriever ìƒì„±\n",
    "query = \"íŒŒì´ì¬ì´ ì²˜ìŒ ê³µê°œëœ ì—°ë„ëŠ”?\"\n",
    "\n",
    "# Dense Retriever ìƒì„±\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type= 'mmr', # default : similarity(ìœ ì‚¬ë„) / mmr ì•Œê³ ë¦¬ì¦˜\n",
    "    search_kwargs={\"k\": 3} # ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ chunkë¥¼ 3ê°œ ê²€ìƒ‰í•˜ê¸° (default : 4)\n",
    ")\n",
    "\n",
    "result_docs = retriever.invoke(query) # ì¿¼ë¦¬ í˜¸ì¶œí•˜ì—¬ retrieverë¡œ ê²€ìƒ‰\n",
    "\n",
    "# 5. ChatPromptTemplate ì •ì˜\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            ë„ˆëŠ” ì¸ê³µì§€ëŠ¥ ì±—ë´‡ìœ¼ë¡œ, ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì •í™•í•˜ê²Œ ì´í•´í•´ì„œ ë‹µë³€ì„ í•´ì•¼í•´.\n",
    "            ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ìœ¼ë¡œë§Œ ë‹µë³€í•˜ê³  ë‚´ìš©ì´ ì—†ë‹¤ë©´, ì˜ ëª¨ë¥´ê² ë‹¤ê³  ë‹µë³€í•´.\n",
    "            ---\n",
    "            CONTEXT:\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 6. LLMChain ì •ì˜\n",
    "llm = ChatUpstage()\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 7. ì§ˆë¬¸ ë° ë‹µë³€\n",
    "response = chain.invoke({\"context\": result_docs, \"input\": query})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4 Open Source Tool(RAGAS)ë¥¼ ì‚¬ìš©í•œ RAG Evaluation\n",
    "\n",
    "- RAGAS í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ RAGì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "- ê²€ìƒ‰ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ë©° Precisionê³¼ Recallì„ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
    "- chunk_sizeì™€ chunk_overlapì— ë”°ë¥¸ RAGì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "**í™œìš© ë©”íŠ¸ë¦­**\n",
    "- context_precision: ê²€ìƒ‰í•œ ë¬¸ì„œ ì¤‘ì—ì„œ ì§„ì§œë¡œ ê´€ë ¨ëœ ë¬¸ì„œê°€ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨\n",
    "- context_recall: ì‹¤ì œë¡œ ê´€ë ¨ëœ ë¬¸ì„œ ì¤‘ì—ì„œ ì–¼ë§ˆë‚˜ ë§ì´ ê²€ìƒ‰ì— ì„±ê³µí–ˆëŠ”ì§€\n",
    "- faithfulness: ìƒì„±ëœ ë‹µë³€ì´ ê°€ì§€ê³  ìˆëŠ” ì§€ì‹ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ë’·ë°›ì¹¨ ë˜ëŠ” ì§€ì— ëŒ€í•œ ë¹„ìœ¨\n",
    "- answer_relevancy: ìƒì„±ëœ ë‹µë³€ì´ ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ìˆëŠ” ì§€\n",
    "\n",
    "ë” ìì„¸í•œ ì„¤ëª…ì€ [RAGAS](https://docs.ragas.io/en/stable/) docs ì°¸ê³  ë°”ëë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "layzer = UpstageDocumentParseLoader(\n",
    "    \"6c440f87930babaf248b7991e4810ef287a780d5c62aa81ad9da8fa8faa43f03.pdf\", # ë¶ˆëŸ¬ì˜¬ íŒŒì¼\n",
    "    output_format='html',  # ê²°ê³¼ë¬¼ í˜•íƒœ : HTML\n",
    "    coordinates= False) # ì´ë¯¸ì§€ OCR ì¢Œí‘œê³„ ê°€ì§€ê³  ì˜¤ì§€ ì•Šê¸°\n",
    "\n",
    "# ì•½ 2ë¶„ ì†Œìš”\n",
    "docs = layzer.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# experiment1: chunk_size=1000, chunk_overlap=100\n",
    "ex1_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100)\n",
    "\n",
    "# experiment2: chunk_size=100, chunk_overlap=0\n",
    "ex2_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0)\n",
    "\n",
    "ex1_splits = ex1_text_splitter.split_documents(docs)\n",
    "ex2_splits = ex2_text_splitter.split_documents(docs)\n",
    "\n",
    "print(\"ex1ì˜ chunksì˜ ê°œìˆ˜:\", len(ex1_splits))\n",
    "print(\"ex2ì˜ chunksì˜ ê°œìˆ˜:\", len(ex2_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "ex1_vectorstore = Chroma.from_documents(\n",
    "     documents=ex1_splits, embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    ")\n",
    "ex1_retriever = ex1_vectorstore.as_retriever(\n",
    "    search_type= 'mmr', # default : similarity(ìœ ì‚¬ë„) / mmr ì•Œê³ ë¦¬ì¦˜\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "ex2_vectorstore = Chroma.from_documents(\n",
    "     documents=ex2_splits, embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    ")\n",
    "ex2_retriever = ex2_vectorstore.as_retriever(\n",
    "    search_type= 'mmr', # default : similarity(ìœ ì‚¬ë„) / mmr ì•Œê³ ë¦¬ì¦˜\n",
    "    search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset\n",
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "ex1_data = {\n",
    "    \"question\": [],\n",
    "    \"answer\": [],\n",
    "    \"contexts\": [],\n",
    "    \"ground_truth\": [],\n",
    "}\n",
    "ex2_data = {\n",
    "    \"question\": [],\n",
    "    \"answer\": [],\n",
    "    \"contexts\": [],\n",
    "    \"ground_truth\": [],\n",
    "}\n",
    "\n",
    "llm = ChatUpstage()\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide most correct answer for the given question from the following context.\n",
    "\n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def fill_data(data, question, retr):\n",
    "    results = retr.invoke(question)\n",
    "    context = [doc.page_content for doc in results]\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "    data[\"question\"].append(question)\n",
    "    data[\"answer\"].append(answer)\n",
    "    data[\"contexts\"].append(context)\n",
    "    data[\"ground_truth\"].append(\"\")\n",
    "\n",
    "# ì†”ë¼ ë…¼ë¬¸ì—ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ 10ê°€ì§€ ë¦¬ìŠ¤íŠ¸ì—…\n",
    "questions = [\n",
    "    \"What makes SOLAR 10.7B superior in performance compared to existing LLMs?\",\n",
    "    \"What are the key components of the Depth Up-Scaling (DUS) methodology?\",\n",
    "    \"How does DUS differ from other LLM scaling methods like Mixture-of-Experts, and what are its advantages?\",\n",
    "    \"What are some examples of successfully scaling small models to large LLMs using DUS?\",\n",
    "    \"How does SOLAR 10.7B-Instruct compare to Mixtral-8x7B-Instruct in terms of performance?\",\n",
    "    \"What roles do depthwise scaling and continued pre-training play in the LLM scaling process?\",\n",
    "    \"What are the potential research and application opportunities enabled by SOLAR 10.7B's release under the Apache 2.0 license?\",\n",
    "    \"How does the DUS methodology simplify training and inference in existing LLMs?\",\n",
    "    \"In which NLP tasks does SOLAR 10.7B demonstrate significant performance improvements?\",\n",
    "    \"What are the key differences between SOLAR 10.7B and SOLAR 10.7B-Instruct, and what are their respective use cases?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    fill_data(ex1_data, question, ex1_retriever)\n",
    "    fill_data(ex2_data, question, ex2_retriever)\n",
    "\n",
    "ex1_dataset = Dataset.from_dict(ex1_data)\n",
    "ex2_dataset = Dataset.from_dict(ex2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from ragas.metrics import context_precision, context_recall, faithfulness\n",
    "from ragas import evaluate\n",
    "\n",
    "\n",
    "def ragas_evalate(dataset):\n",
    "    result = evaluate(\n",
    "        dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            faithfulness,\n",
    "        ],\n",
    "        llm=llm,\n",
    "        embeddings=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# chunk_size: 1000, chunk_overlap: 100ìœ¼ë¡œ ìˆ˜í–‰í•  ê²½ìš°\n",
    "ragas_evalate(ex1_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# chunk_size: 100, chunk_overlap: 0ìœ¼ë¡œ ìˆ˜í–‰í•  ê²½ìš°\n",
    "ragas_evalate(ex2_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ ì‹¤ìŠµ ë§ˆë¬´ë¦¬\n",
    "\n",
    " ì´ë²ˆ ì‹¤ìŠµì„ í†µí•´ Upstage Solar ëª¨ë¸ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì„ ë‹¨ê³„ë³„ë¡œ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤.\n",
    "\n",
    " í•™ìŠµì„ í†µí•´ LLMì´ ë¬´ì—‡ì´ê³ , LLMì˜ í•œê³„ì™€ ì´ë¥¼ ë³´ì™„í•˜ëŠ” RAGì˜ ê°œë…, RAGë¥¼ ìœ„í•´ ë¬¸ì„œë¥¼ ì „ì²˜ë¦¬í•˜ê³  ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì´ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ëª¨ë“  ê³¼ì •ì„ ë‹¤ë¤„ ë³´ì…¨ìŠµë‹ˆë‹¤. ë˜í•œ RAGAS ë¼ê³  í•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì„ í‰ê°€í•˜ëŠ” ë°©ë²•ê¹Œì§€ ë‹¤ë£¨ì–´ë³´ì•˜ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ë¥¼ í†µí•´ ì—¬ëŸ¬ë¶„ì´ ê°ì í”„ë¡œì íŠ¸ì—ì„œ ì„¤ê³„í•œ ì„œë¹„ìŠ¤ì— ì í•©í•œ RAG ì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ê³  êµ¬í˜„í•  ìˆ˜ ìˆëŠ” ê¸°ì´ˆë¥¼ ë‹¤ì§ˆ ìˆ˜ ìˆê¸°ë¥¼ ë°”ëë‹ˆë‹¤. Domain-specificí•œ RAG ì‹œìŠ¤í…œì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” PDF ë¬¸ì„œì™€ ì›¹í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì„ ì—°ìŠµí•´ì„œ ë³´ë‹¤ ë§ì€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ì—°ìŠµì„ í•´ë‘ì‹œë©´ ë§ì€ ë„ì›€ì´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
