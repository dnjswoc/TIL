{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Objectives**\n",
    "\n",
    "1. 실습 개요\n",
    "    - RAG (Retrieval-Augmented Generation)의 개념을 이해하고 활용법을 학습.\n",
    "    - Upstage의 Solar LLM을 기반으로 단계별 RAG 시스템을 구축.\n",
    "    - RAG 시스템 설계와 구현을 통해 언어 모델의 응용 능력 강화.\n",
    "\n",
    "2. 실습 진행 목적 및 배경\n",
    "    - 목적: RAG 개념과 Solar LLM을 활용하여 정보 검색과 언어 생성을 결합한 실무형 시스템 구현 역량을 강화합니다.\n",
    "    - 배경: RAG는 언어 모델과 정보 검색 기술의 융합으로 복잡한 문제 해결에 활용되며, 실제 실무에서 많이 활용됩니다. 이를 통해, 실제 실무에서 적용할 수 있는 역량을 강화합니다.\n",
    "\n",
    "3. 실습 수행으로 얻어갈 수 있는 역량\n",
    "    - RAG 개념을 이해하고 이를 실제로 구현하는 능력.\n",
    "    - LangChain을 활용한 LLM Chain 설계 및 챗봇 개발 기술.\n",
    "\n",
    "4. 실습 핵심 내용\n",
    "    - LLM Chain 구현: LangChain을 활용하여 간단한 ChatBot을 설계하고 구현.\n",
    "    - RAG 시스템 구축: RAG의 개념을 이해하고 이를 단계적으로 설계 및 구현.\n",
    "    - LLM API 활용: Upstage의 Solar LLM API를 통해 LLM을 호출하고 시스템에 통합.\n",
    "    - 실무 프로젝트 기반 학습: 실질적인 프로젝트 설계를 통해 실무 적용 가능성 향상.   \n",
    "\n",
    "⭐ 이번에는 여러분들만의 서비스 기획안에 따라 데이터를 바꾸어 진행해보세요. 본 실습은 프로젝트 예시로 작성된 제품 매뉴얼 문서를 기반으로 진행됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prerequisties**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "langchain == 0.3.12\n",
    "langchain-chroma == 0.1.4\n",
    "langchain-core == 0.3.25\n",
    "langchain-openai == 0.2.12\n",
    "langchain-text-splitters == 0.3.3\n",
    "langchain-upstage == 0.4.0\n",
    "getpass4 == 0.0.14.1\n",
    "openai == 1.57.4\n",
    "ragas == 0.2.8\n",
    "langchain_community == 0.3.12\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 약 1분 소요\n",
    "!pip install -qU openai langchain langchain-upstage langchain-chroma getpass4 ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAGAS callback 함수 수정\n",
    "- 경로: `/usr/local/lib/python3.10/dist-packages/ragas/callbacks.py`\n",
    "- ragas를 다운로드 받고, 위 파일 경로의 값을 아래 코드로 바꿔끼웁니다.\n",
    "- 실행한 후 세션을 다시 시작합니다. 세션을 다시 시작한 후 아래 set API key부터 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import typing as t\n",
    "import uuid\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    BaseCallbackHandler,\n",
    "    CallbackManager,\n",
    "    CallbackManagerForChainGroup,\n",
    "    CallbackManagerForChainRun,\n",
    "    Callbacks,\n",
    ")\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "def new_group(\n",
    "    name: str,\n",
    "    inputs: t.Dict,\n",
    "    callbacks: Callbacks,\n",
    "    tags: t.Optional[t.List[str]] = None,\n",
    "    metadata: t.Optional[t.Dict[str, t.Any]] = None,\n",
    ") -> t.Tuple[CallbackManagerForChainRun, CallbackManagerForChainGroup]:\n",
    "    tags = tags or []\n",
    "    metadata = metadata or {}\n",
    "\n",
    "    # start evaluation chain\n",
    "    if isinstance(callbacks, list):\n",
    "        cm = CallbackManager.configure(inheritable_callbacks=callbacks)\n",
    "    else:\n",
    "        cm = t.cast(CallbackManager, callbacks)\n",
    "    cm.tags = tags\n",
    "    cm.metadata = metadata\n",
    "    rm = cm.on_chain_start({\"name\": name}, inputs)\n",
    "    child_cm = rm.get_child()\n",
    "    group_cm = CallbackManagerForChainGroup(\n",
    "        child_cm.handlers,\n",
    "        child_cm.inheritable_handlers,\n",
    "        child_cm.parent_run_id,\n",
    "        parent_run_manager=rm,\n",
    "        tags=child_cm.tags,\n",
    "        inheritable_tags=child_cm.inheritable_tags,\n",
    "        metadata=child_cm.metadata,\n",
    "        inheritable_metadata=child_cm.inheritable_metadata,\n",
    "    )\n",
    "\n",
    "    return rm, group_cm\n",
    "\n",
    "\n",
    "class ChainType(Enum):\n",
    "    EVALUATION = \"evaluation\"\n",
    "    METRIC = \"metric\"\n",
    "    ROW = \"row\"\n",
    "    RAGAS_PROMPT = \"ragas_prompt\"\n",
    "\n",
    "\n",
    "class ChainRun(BaseModel):\n",
    "    run_id: str\n",
    "    parent_run_id: t.Optional[str]\n",
    "    name: str\n",
    "    inputs: t.Dict[str, t.Any]\n",
    "    metadata: t.Dict[str, t.Any]\n",
    "    outputs: t.Dict[str, t.Any] = Field(default_factory=dict)\n",
    "    children: t.List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class ChainRunEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, uuid.UUID):\n",
    "            return str(o)\n",
    "        if isinstance(o, ChainType):\n",
    "            return o.value\n",
    "        # if isinstance(o, EvaluationResult):\n",
    "        #     return \"\"\n",
    "        return json.JSONEncoder.default(self, o)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RagasTracer(BaseCallbackHandler):\n",
    "    traces: t.Dict[str, ChainRun] = field(default_factory=dict)\n",
    "\n",
    "    def on_chain_start(\n",
    "        self,\n",
    "        serialized: t.Dict[str, t.Any],\n",
    "        inputs: t.Dict[str, t.Any],\n",
    "        *,\n",
    "        run_id: uuid.UUID,\n",
    "        parent_run_id: t.Optional[uuid.UUID] = None,\n",
    "        tags: t.Optional[t.List[str]] = None,\n",
    "        metadata: t.Optional[t.Dict[str, t.Any]] = None,\n",
    "        **kwargs: t.Any,\n",
    "    ) -> t.Any:\n",
    "        self.traces[str(run_id)] = ChainRun(\n",
    "            run_id=str(run_id),\n",
    "            parent_run_id=str(parent_run_id) if parent_run_id else None,\n",
    "            name=serialized[\"name\"],\n",
    "            inputs=inputs,\n",
    "            metadata=metadata or {},\n",
    "            children=[],\n",
    "        )\n",
    "\n",
    "        if parent_run_id and str(parent_run_id) in self.traces:\n",
    "            self.traces[str(parent_run_id)].children.append(str(run_id))\n",
    "\n",
    "    def on_chain_end(\n",
    "        self,\n",
    "        outputs: t.Dict[str, t.Any],\n",
    "        *,\n",
    "        run_id: uuid.UUID,\n",
    "        **kwargs: t.Any,\n",
    "    ) -> t.Any:\n",
    "        self.traces[str(run_id)].outputs = outputs\n",
    "\n",
    "    def to_jsons(self) -> str:\n",
    "        return json.dumps(\n",
    "            [t.model_dump() for t in self.traces.values()],\n",
    "            cls=ChainRunEncoder,\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MetricTrace(dict):\n",
    "    scores: t.Dict[str, float] = field(default_factory=dict)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.scores.__repr__()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "def parse_run_traces(\n",
    "    traces: t.Dict[str, ChainRun],\n",
    "    parent_run_id: t.Optional[str] = None,\n",
    ") -> t.List[t.Dict[str, t.Any]]:\n",
    "\n",
    "    root_traces = [\n",
    "        chain_trace\n",
    "        for chain_trace in traces.values()\n",
    "        if chain_trace.parent_run_id == parent_run_id\n",
    "    ]\n",
    "\n",
    "    if len(root_traces) > 1:\n",
    "        raise ValueError(\n",
    "            \"Multiple root traces found! This is a bug on our end, please file an issue and we will fix it ASAP :)\"\n",
    "        )\n",
    "    root_trace = root_traces[0]\n",
    "\n",
    "    # get all the row traces\n",
    "    parased_traces = []\n",
    "    for row_uuid in root_trace.children:\n",
    "        row_trace = traces[row_uuid]\n",
    "        metric_traces = MetricTrace()\n",
    "        for metric_uuid in row_trace.children:\n",
    "            metric_trace = traces[metric_uuid]\n",
    "            metric_traces.scores[metric_trace.name] = metric_trace.outputs.get(\n",
    "                \"output\", {}\n",
    "            )\n",
    "            # get all the prompt IO from the metric trace\n",
    "            prompt_traces = {}\n",
    "            for i, prompt_uuid in enumerate(metric_trace.children):\n",
    "                prompt_trace = traces[prompt_uuid]\n",
    "                output = prompt_trace.outputs.get(\"output\", {})\n",
    "                output = output[0] if isinstance(output, list) else output\n",
    "                prompt_traces[f\"{prompt_trace.name}\"] = {\n",
    "                    \"input\": prompt_trace.inputs.get(\"data\", {}),\n",
    "                    \"output\": output,\n",
    "                }\n",
    "            metric_traces[f\"{metric_trace.name}\"] = prompt_traces\n",
    "        parased_traces.append(metric_traces)\n",
    "\n",
    "    return parased_traces\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Get the Upstage API key using getpass\n",
    "try:\n",
    "    if \"UPSTAGE_API_KEY\" not in os.environ or not os.environ[\"UPSTAGE_API_KEY\"]:\n",
    "        os.environ[\"UPSTAGE_API_KEY\"] = getpass.getpass(\"Enter your Upstage API key: \")\n",
    "\n",
    "    print(\"API key has been set successfully.\")\n",
    "\n",
    "except:\n",
    "    print(\"Something wrong with your API KEY. Check your API Console again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise Overview**\n",
    "## **실습 목차**\n",
    "-  **RAG 파이프라인 단계별 기법 소개**:  RAG 개념을 이해하고 단계별로 RAG를 직접 구현해보자<br>\n",
    "    - 1-1 What is RAG?\n",
    "    - 1-2 단계별 RAG 구현해보기\n",
    "      - 1-2-1 문서 전처리 (Document Preprocessing) <br>\n",
    "        (1) Loading Documents by `Upstage Document Parse API` <br>\n",
    "        (2) Text Splitting <br>\n",
    "        (3) Text Embedding  by `Upstage Solar Embedding API` <br>\n",
    "        (4) Vectorstore <br>\n",
    "      - 1-2-2 RAG 실행하기  <br>\n",
    "       (1) Retrieving Related Chunks from the Vectorstore <br>\n",
    "       (2) Creating a Prompt with Retrieved Results <br>\n",
    "       (3) Implementing an LLM Chain <br>\n",
    "       (4) Executing the LLM Chain and Generating the Response\n",
    "    - 1-3 Web에서 데이터 가져와서 RAG 구축 연습해보기\n",
    "    - 1-4 Open Source Tool(RAGAS)를 사용한 RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1 What is RAG\n",
    "\n",
    "![Overview](https://drive.google.com/uc?id=18nYSFhTdlhmlXGEaiENEzfJL9N6v23XI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📘  RAG 란?\n",
    "\n",
    "- 정의 :  RAG(Retrieval-Augmented Generation)는 **검색 엔진**과 **LLM(대규모 언어 모델)**을 결합한 기술로, 주어진 쿼리에 대해 정보를 검색하고 검색된 정보를 활용하여 LLM이 더 정확하고 맥락에 맞는 응답을 생성할 수 있도록 도와줍니다.\n",
    "- 방식\n",
    "  - 이 기술은 LLM이 보유하지 않는 정보를 보충하기 위해 외부 문서를 검색하여 이를 기반으로 응답을 생성합니다.\n",
    "  - 더 구체적으로는 외부 문서를 로드하고, 전처리 단계를 통해서 이를 조각(chunks)으로 분리한 후, 쿼리와 관련된 조각을 가져와 LLM 프롬프트에 포함하는 방식으로 외부 정보를 LLM에게 제공한 후, LLM이 응답을 생성합니다.\n",
    "- 기대효과 : LLM이 자체적으로 보유하지 않는 정보를 외부 문서를 통해 보충해주기에, LLM의 한계인 Hallucination과 Knowledge Cutoff 문제를 해결할 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📘 각 단계별 설명\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1hwyxaRToKo7JbaupBctR0KIKOgz3jOsm\" alt=\"Definition of DP\" width=\"800\" />\n",
    "\n",
    "1. **문서 전처리 (Document Preprocessing)**:\n",
    "   1. **문서 로드 (Loading Documents)**: `Upstage Document Parse API`를 사용해 외부 PDF 문서를 HTML 텍스트 데이터 형태로 로드합니다.\n",
    "   2. **텍스트 분할 (Text Splitting)**: 긴 문서를 작은 조각(chunks)으로 분할하여 검색 효율성을 높입니다. 이를 통해 모델이 보다 구체적이고 정확하게 정보를 찾을 수 있습니다.\n",
    "   3.  **임베딩 (Embedding)**: `Upstage Solar Embedding API`를 사용해 텍스트를 벡터로 변환합니다. 이 작업은 문서의 의미/문맥을 벡터 공간에 매핑하여 모델이 이를 검색 및 분석에 활용할 수 있도록 합니다.\n",
    "   4. **벡터스토어 (Vectorstore)**: 임베딩된 벡터를 저장하는 데이터베이스로, RAG 시스템 내에서 검색할 수 있는 공간을 제공합니다.\n",
    "\n",
    "2. **RAG 실행 (Implementing RAG)**:\n",
    "   5. **관련 조각 검색 (Retrieving Related Chunks)**: 벡터 스토어에서 쿼리와 관련된 조각을 검색해 가져옵니다. 이를 통해 LLM이 응답을 생성할 때 필요한 정보를 제공합니다.\n",
    "   6. **프롬프트 생성 (Creating a Prompt with Retrieved Results)**: 검색된 조각을 프롬프트에 포함시켜 LLM이 문맥을 이해하고 응답할 수 있도록 합니다.\n",
    "   7. **LLM 체인 구현 (Implementing an LLM Chain)**: 위의 입력 프롬프트를 기반으로 LLM 체인을 정의합니다.\n",
    "   8. **LLM 체인 실행 및 응답 출력 (Executing the LLM Chain and Generating the Response)**: LLM 체인을 실행하여 최종 응답을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2-1 문서 전처리 (Docuemnt Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Loading Documents by `Upstage Document Parse API`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Upstage Document Parse API`를 사용해 외부 PDF 문서를 HTML 텍스트 데이터 형태로 로드합니다.\n",
    "\n",
    "- Upstage Document Parse extracts layouts, tables, and figures from any document\n",
    "- LangChain provides powerful tools for text splitting and vectorization\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1vw9GqpzZDrAAfNdUL_ko7-VvJlRZSDou\" alt=\"Definition of DP\" width=\"600\" />\n",
    "\n",
    "**정의**\n",
    "- `Upstage Document Parse`는 복잡한 문서를 LLM이 이해할 수 있는 형식으로 변환해주는 도구\n",
    "- LLM은 질의응답 시스템(QA), 문서 요약 등의 자연어 task에는 유용하지만, 문서 파일을 바로 읽고 처리할 수는 없습니다.\n",
    "\n",
    "**작동 방식**\n",
    "- 문서 파일을 HTML이나 Markdown과 같은 LLM이 이해할 수 있는 형식으로 변환합니다.\n",
    "- 문서의 레이아웃, 표 형식, 이미지 등 다양한 요소들을 모두 반영하여 변환을 진행합니다.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1yAbnaXDI6t_wTTV1i03GDqqoanN24bDv\" alt=\"DP\" width=\"600\" />\n",
    "\n",
    "**성능**\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1PfgSNDsWr8RgBua7fO5eIDsq43B_W2QH\" alt=\"Definition of DP\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 실습을 진행할 파일은 Galaxy A12 매뉴얼 문서입니다.\n",
    "!wget -O galaxy_A12.pdf https://files.customersaas.com/files/AOVV-atOIDUSB5NELiKGDtk8.pdf\n",
    "\n",
    "# 실제로 진행할 파일은 아래 방식으로 로드하여 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 📁 다운로드 된 파일 확인하는 방법\n",
    "\n",
    "1. Colab 화면 왼쪽 파일 이모지를 클릭해보면 아래 그림처럼 galaxy_A12.pdf가 잘 다운로드 된 것을 확인할 수 있습니다.\n",
    "\n",
    "2. 만약 위 과정을 실패하셨거나 다른 PDF 문서를 직접 업로드 후 확인해보고 싶으시다면, 상단의 첫 번째 버튼 클릭 후 가능합니다.\n",
    "\n",
    "- [Galaxy_A12 메뉴얼 PDF 링크](https://files.customersaas.com/files/AOVV-atOIDUSB5NELiKGDtk8.pdf) 에서 로컬에 직접 다운로드 받으신 후 업로드\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1p3PoZW4r_v_dpNsSypZYJU-Lp0dlaJdk\" alt=\"Colab File Emoji\" width=\"400\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Document Parse로 다운로드 된 문서 불러오기\n",
    "%%time\n",
    "\n",
    "from langchain_upstage import UpstageDocumentParseLoader\n",
    "\n",
    "layzer = UpstageDocumentParseLoader(\n",
    "    \"galaxy_A12.pdf\", # 불러올 파일\n",
    "    output_format='html',  # 결과물 형태 : HTML\n",
    "    coordinates= False) # 이미지 OCR 좌표계 가지고 오지 않기\n",
    "\n",
    "docs = layzer.load()\n",
    "\n",
    "# layzer.lazy_load()\n",
    "# 메모리 효율을 향상시키기 위해서, lazy_load() 로 페이지별로 문서를 불러올 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# HTML 형태 확인해보기\n",
    "for doc in docs:\n",
    "    print(doc.page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 불러온 HTML 텍스트 데이터를 이미지 형태로 확인해보기\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(docs[0].page_content[:2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Text Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**📘Text Splitting을 하는 이유**\n",
    "- 만약, 문서가 길지 않고 짧다면 그대로 넣어주도 무방함.\n",
    "- 하지만, 문서의 사이즈가 큰 경우\n",
    "  - LLM에게 입력할 수 있는 최대 토큰 수를 초과\n",
    "  - 쿼리와 연관되지 않은 불필요한 부분이 포함되어 헷갈리게 함으로써 성능이 잘 나오지 않음.\n",
    "- 따라서  긴 문서를 작은 조각(chunks)으로 분할하여 검색 효율성을 높입니다. 이를 통해 모델이 보다 구체적이고 정확하게 정보를 찾을 수 있습니다.\n",
    "\n",
    "<b>📘RecursiveCharacterTextSplitter </b>\n",
    "- 가장 대표적으로 많이 활용되는 Text Splitter\n",
    "- **방식**\n",
    "  - `RecursiveCharacterTextSplitter` : 글자수가 `chunk size`를 넘지 않을 때까지 아래와 같이 text를 분리함.\n",
    "  -  `(['\\n\\n', '\\n', ' ', ''])` : 문단, 문장, 단어 순서로 문서를 분리하다가 chunk 내 글자수가 `chunk size` 보다 작아지면 분리함.\n",
    "- **파라미터**\n",
    "  - `chunk_size` : 하나의 chunk 당 최대 글자 수\n",
    "  - `chunk_overlap` : 각 chunk 마다 겹치는 글자 수\n",
    "    - chunk 마다 일부 내용을 겹쳐서 분할된 chuck 마다 맥락이 이어질수 있도록 하기 위함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# 2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(\"Splits:\", len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# split된 결과 확인 : chunk size 1000로 대다수가 분리됨.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "split_lengths = [len(split.page_content) for split in splits]\n",
    "\n",
    "# Create a bar graph\n",
    "plt.bar(range(len(split_lengths)), split_lengths)\n",
    "plt.title(\"RecursiveCharacterTextSplitter\")\n",
    "plt.xlabel(\"Split Index\")\n",
    "plt.ylabel(\"Split Content Length\")\n",
    "plt.xticks(range(len(split_lengths)), [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1dcmr0Q572fBwY-o4GN5mhCCise8WdN7C\" alt=\"solar embedding\" width=\"600\" />\n",
    "\n",
    "**Embedding 정의**\n",
    "- Embedding은 **자연어(텍스트) 데이터**를 컴퓨터가 이해할 수 있는 **숫자** 형식으로 변환하는 것.\n",
    "- 사람이 이해하는 텍스트 정보를 컴퓨터가 이해할 수 있는 형식(=Vector)으로 변환하여 데이터를 LLM 등 모델에 쉽게 전달할 수 있도록 합니다.\n",
    "\n",
    "**`Upstage Solar Embedding API`**\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=18tPtT8_6M9ilT7q5FkBx9DGYMHj8yRIl\" alt=\"solar embedding\" width=\"600\" />\n",
    "\n",
    "- 성능 : 한국어/영어/일본어 모두 널리 사용되는 임베딩 모델보다 성능이 뛰어남\n",
    "\n",
    "**RAG에서 Embedding은 VectorStore 에서 모델을 지정하여 활용할 수 있습니다**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 벡터스토어 (VectorStore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**벡터**를 저장하는 데이터베이스로, RAG 시스템 내에서 검색할 수 있는 공간을 제공합니다.\n",
    "- VectorStore마다 지원하는 검색 알고리즘이 달라짐.\n",
    "- Vectorstore 유형\n",
    "  -  로컬 : 내 컴퓨터에 저장\n",
    "  -  Cloud : 각 DB 회사 서버에 저장\n",
    "- Langchain이 제공하는 더 다양한 VectorStore : [링크](https://python.langchain.com/v0.2/docs/integrations/vectorstores/)\n",
    "\n",
    "**ChromaDB**\n",
    "- Cloud 기반 오픈소스 벡터 데이터베이스\n",
    "- 임베딩 Vector 저장 및 검색 기능\n",
    "- 문서 추가 및 업데이트, 삭제\n",
    "- 다양한 검색 기능 제공\n",
    "- Langchain과 통합되어 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# 3. Embed & indexing\n",
    "vectorstore = Chroma.from_documents(\n",
    "     documents= splits, embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**참고**\n",
    "- [Chroma LangChain Documentation](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)\n",
    "- [Chroma Official Documentation](https://docs.trychroma.com/getting-started)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###(Recap.)  OVERVIEW OF RAG\n",
    "\n",
    "- 외부 문서를 바탕으로 문서 전처리까지 완료\n",
    "- 이제 각 chunk 단위로 분리된 문서 중 \"사용자의 질문(쿼리)\"와 관련한 chunk를 **검색**해보자 !\n",
    "\n",
    "![Overview](https://drive.google.com/uc?id=18nYSFhTdlhmlXGEaiENEzfJL9N6v23XI)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2-2 RAG 실행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) Retrieving Related Chunks\n",
    "\n",
    "- 벡터스토어에서 사용자의 질문(쿼리)와 유사한 문서를 검색하는 과정입니다. LLM이 응답을 생성할 때 필요한 정보를 제공합니다.\n",
    "\n",
    "- Retriever 유형\n",
    "  - **Sparse Retriever**: 키워드 검색\n",
    "    - 쿼리를 키워드 벡터로 전환하여 키워드 기반 검색을 진행합니다.\n",
    "    - 특정 도메인 지식(e.g. 의학, 법률) 등을 검색할 때 용이합니다.\n",
    "    - 단어들이 직접적으로 유사한 상황에 용이합니다.\n",
    "    - 대표 알고리즘 : TF-IDF, BM25\n",
    "  - **Dense Retriever**: 의미 검색\n",
    "    - 쿼리를 벡터로 임베딩하여 가장 유사도가 높은 chunk를 찾습니다.\n",
    "    - 복잡한 자연어 질문에 대한 검색 시 용이합니다.\n",
    "    - 문맥적 유사도가 중요한 상황에 용이합니다.\n",
    "    - 대표 알고리즘 : FAISS\n",
    "\n",
    "**참고** <br>\n",
    "- [Lanchain - Retriever](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/)  documentation을 통해 더 자세한 내용 참고 바랍니다\n",
    "- [MMR 알고리즘](https://wikidocs.net/231585) 에 대한 설명\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 사용자의 질문, 쿼리\n",
    "query = \"Please tell me about galaxy a12\"\n",
    "\n",
    "# Dense Retriever 생성\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type= 'mmr', # default : similarity(유사도) / mmr 알고리즘\n",
    "    search_kwargs={\"k\": 3} # 쿼리와 관련된 chunk를 3개 검색하기 (default : 4)\n",
    ")\n",
    "\n",
    "result_docs = retriever.invoke(query) # 쿼리 호출하여 retriever로 검색\n",
    "\n",
    "print(len(result_docs))\n",
    "print(result_docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) Creating a Prompt with Retrieved Results\n",
    "- 검색된 문서 chunk들을 프롬프트에 포함시켜 LLM이 문맥을 이해하고 응답할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# context에 검색된 chunk들을 넣어줍니다.\n",
    "# (지난 시간에 배웠던) LLM Chain 구성하는 법.\n",
    "# 1. llm 정의, 2. prompt 정의, 3. chain 정의, 4. chain 호출\n",
    "\n",
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are an assistant for question-answering tasks.\n",
    "            Use the following pieces of retrieved context to answer the question considering the history of the conversation.\n",
    "            If you don't know the answer, just say that you don't know.\n",
    "            ---\n",
    "            CONTEXT:\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) Implementing an LLM Chain\n",
    "- 위의 입력 프롬프트를 기반으로 LLM 체인을 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (8) Executing the LLM Chain and Generating the Response\n",
    "-  LLM 체인을 실행하여 최종 응답을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query = \"Please tell me about galaxy a12\"\n",
    "\n",
    "response = chain.invoke({\"context\": result_docs, \"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "response  # RAG + LLM -> natural한 resopnse!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3 Web에서 데이터 불러와서 RAG 구축 연습해보기\n",
    "\n",
    "- langchain_community에 있는 WebBaseLoader를 이용하여 wikipedia의 내용을 불러와서 RAG를 구축하는 예제를 연습해봅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# 1. 웹 페이지 로드\n",
    "loader = WebBaseLoader(\"https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%8D%AC\")\n",
    "data = loader.load()\n",
    "\n",
    "# 2. 텍스트 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "# 3. 임베딩 및 벡터 저장소 생성\n",
    "vectorstore = Chroma.from_documents(\n",
    "     documents= splits, embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    ")\n",
    "\n",
    "# 4. Dense Retriever 생성\n",
    "query = \"파이썬이 처음 공개된 연도는?\"\n",
    "\n",
    "# Dense Retriever 생성\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type= 'mmr', # default : similarity(유사도) / mmr 알고리즘\n",
    "    search_kwargs={\"k\": 3} # 쿼리와 관련된 chunk를 3개 검색하기 (default : 4)\n",
    ")\n",
    "\n",
    "result_docs = retriever.invoke(query) # 쿼리 호출하여 retriever로 검색\n",
    "\n",
    "# 5. ChatPromptTemplate 정의\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            너는 인공지능 챗봇으로, 주어진 문서를 정확하게 이해해서 답변을 해야해.\n",
    "            문서에 있는 내용으로만 답변하고 내용이 없다면, 잘 모르겠다고 답변해.\n",
    "            ---\n",
    "            CONTEXT:\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 6. LLMChain 정의\n",
    "llm = ChatUpstage()\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 7. 질문 및 답변\n",
    "response = chain.invoke({\"context\": result_docs, \"input\": query})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4 Open Source Tool(RAGAS)를 사용한 RAG Evaluation\n",
    "\n",
    "- RAGAS 프레임워크를 활용하여 RAG의 성능을 평가합니다.\n",
    "- 검색의 성능을 평가하며 Precision과 Recall을 측정합니다.\n",
    "- chunk_size와 chunk_overlap에 따른 RAG의 성능을 비교합니다.\n",
    "\n",
    "**활용 메트릭**\n",
    "- context_precision: 검색한 문서 중에서 진짜로 관련된 문서가 차지하는 비율\n",
    "- context_recall: 실제로 관련된 문서 중에서 얼마나 많이 검색에 성공했는지\n",
    "- faithfulness: 생성된 답변이 가지고 있는 지식으로 얼마나 뒷받침 되는 지에 대한 비율\n",
    "- answer_relevancy: 생성된 답변이 주어진 질문과 얼마나 관련성이 있는 지\n",
    "\n",
    "더 자세한 설명은 [RAGAS](https://docs.ragas.io/en/stable/) docs 참고 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "layzer = UpstageDocumentParseLoader(\n",
    "    \"6c440f87930babaf248b7991e4810ef287a780d5c62aa81ad9da8fa8faa43f03.pdf\", # 불러올 파일\n",
    "    output_format='html',  # 결과물 형태 : HTML\n",
    "    coordinates= False) # 이미지 OCR 좌표계 가지고 오지 않기\n",
    "\n",
    "# 약 2분 소요\n",
    "docs = layzer.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# experiment1: chunk_size=1000, chunk_overlap=100\n",
    "ex1_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100)\n",
    "\n",
    "# experiment2: chunk_size=100, chunk_overlap=0\n",
    "ex2_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0)\n",
    "\n",
    "ex1_splits = ex1_text_splitter.split_documents(docs)\n",
    "ex2_splits = ex2_text_splitter.split_documents(docs)\n",
    "\n",
    "print(\"ex1의 chunks의 개수:\", len(ex1_splits))\n",
    "print(\"ex2의 chunks의 개수:\", len(ex2_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "ex1_vectorstore = Chroma.from_documents(\n",
    "     documents=ex1_splits, embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    ")\n",
    "ex1_retriever = ex1_vectorstore.as_retriever(\n",
    "    search_type= 'mmr', # default : similarity(유사도) / mmr 알고리즘\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "ex2_vectorstore = Chroma.from_documents(\n",
    "     documents=ex2_splits, embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    ")\n",
    "ex2_retriever = ex2_vectorstore.as_retriever(\n",
    "    search_type= 'mmr', # default : similarity(유사도) / mmr 알고리즘\n",
    "    search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset\n",
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "ex1_data = {\n",
    "    \"question\": [],\n",
    "    \"answer\": [],\n",
    "    \"contexts\": [],\n",
    "    \"ground_truth\": [],\n",
    "}\n",
    "ex2_data = {\n",
    "    \"question\": [],\n",
    "    \"answer\": [],\n",
    "    \"contexts\": [],\n",
    "    \"ground_truth\": [],\n",
    "}\n",
    "\n",
    "llm = ChatUpstage()\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide most correct answer for the given question from the following context.\n",
    "\n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def fill_data(data, question, retr):\n",
    "    results = retr.invoke(question)\n",
    "    context = [doc.page_content for doc in results]\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "    data[\"question\"].append(question)\n",
    "    data[\"answer\"].append(answer)\n",
    "    data[\"contexts\"].append(context)\n",
    "    data[\"ground_truth\"].append(\"\")\n",
    "\n",
    "# 솔라 논문에서 나올 수 있는 질문 10가지 리스트업\n",
    "questions = [\n",
    "    \"What makes SOLAR 10.7B superior in performance compared to existing LLMs?\",\n",
    "    \"What are the key components of the Depth Up-Scaling (DUS) methodology?\",\n",
    "    \"How does DUS differ from other LLM scaling methods like Mixture-of-Experts, and what are its advantages?\",\n",
    "    \"What are some examples of successfully scaling small models to large LLMs using DUS?\",\n",
    "    \"How does SOLAR 10.7B-Instruct compare to Mixtral-8x7B-Instruct in terms of performance?\",\n",
    "    \"What roles do depthwise scaling and continued pre-training play in the LLM scaling process?\",\n",
    "    \"What are the potential research and application opportunities enabled by SOLAR 10.7B's release under the Apache 2.0 license?\",\n",
    "    \"How does the DUS methodology simplify training and inference in existing LLMs?\",\n",
    "    \"In which NLP tasks does SOLAR 10.7B demonstrate significant performance improvements?\",\n",
    "    \"What are the key differences between SOLAR 10.7B and SOLAR 10.7B-Instruct, and what are their respective use cases?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    fill_data(ex1_data, question, ex1_retriever)\n",
    "    fill_data(ex2_data, question, ex2_retriever)\n",
    "\n",
    "ex1_dataset = Dataset.from_dict(ex1_data)\n",
    "ex2_dataset = Dataset.from_dict(ex2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from ragas.metrics import context_precision, context_recall, faithfulness\n",
    "from ragas import evaluate\n",
    "\n",
    "\n",
    "def ragas_evalate(dataset):\n",
    "    result = evaluate(\n",
    "        dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            faithfulness,\n",
    "        ],\n",
    "        llm=llm,\n",
    "        embeddings=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# chunk_size: 1000, chunk_overlap: 100으로 수행할 경우\n",
    "ragas_evalate(ex1_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# chunk_size: 100, chunk_overlap: 0으로 수행할 경우\n",
    "ragas_evalate(ex2_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 실습 마무리\n",
    "\n",
    " 이번 실습을 통해 Upstage Solar 모델을 활용하여 RAG 시스템을 단계별로 구현하는 방법을 배웠습니다.\n",
    "\n",
    " 학습을 통해 LLM이 무엇이고, LLM의 한계와 이를 보완하는 RAG의 개념, RAG를 위해 문서를 전처리하고 관련 문서를 검색하여 이를 바탕으로 LLM이 응답을 생성하는 모든 과정을 다뤄 보셨습니다. 또한 RAGAS 라고 하는 오픈소스 라이브러리를 활용하여 RAG 시스템을 평가하는 방법까지 다루어보았습니다.\n",
    "\n",
    "이를 통해 여러분이 각자 프로젝트에서 설계한 서비스에 적합한 RAG 시스템을 설계하고 구현할 수 있는 기초를 다질 수 있기를 바랍니다. Domain-specific한 RAG 시스템에서 가장 많이 사용되는 PDF 문서와 웹페이지를 가져오는 것을 연습해서 보다 많은 데이터를 처리할 수 있게 연습을 해두시면 많은 도움이 됩니다.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
