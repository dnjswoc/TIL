{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Objectives**\n",
    "\n",
    "1. 실습 개요\n",
    "    - LangChain의 개념을 이해하고 활용법을 학습.\n",
    "    - Upstage의 Solar LLM을 기반으로 LangChain의 ChatModel을 단계별로 구축.\n",
    "    - AI 기반 챗봇 개발 실습을 통해 실질적인 프로젝트 구현 능력 강화.\n",
    "    - ⭐ Prompt Engineering 기법을 학습하고, 실제 시스템 프롬프트 구현에 적용.\n",
    "\n",
    "\n",
    "\n",
    "2. 실습 진행 목적 및 배경\n",
    "    - 목적: LangChain과 Upstage Solar LLM을 활용해 AI 기반 언어 모델의 실질적인 응용 능력과 챗봇 개발 역량을 강화합니다.\n",
    "    - 배경:\n",
    "        - LLM은 다양한 산업에서 핵심 기술로 자리 잡고 있으며, LangChain은 이를 연결하고 확장하는 데 강력한 프레임워크로 활용됩니다. 이러한 Langchain 프레임워크를 학습하여 실무에 적용 가능한 역량을 강화합니다.\n",
    "        - ⭐ Prompt Engineering은 LLM의 성능을 극대화하기 위한 필수 기술입니다. 다양한 Prompt Engineering 방법론을 학습하여 실무에 적용 가능한 역량을 강화합니다.\n",
    "\n",
    "\n",
    "3. 실습 수행으로 얻어갈 수 있는 역량\n",
    "    - AI 모델 API 활용 능력: Upstage Console에서 API Key를 발급받아 활용하고, Solar LLM을 호출하는 능력을 습득합니다.\n",
    "    - LangChain 이해 및 활용 능력: LangChain의 구조와 원리를 이해하고, 이를 바탕으로 LLM Chain을 설계하고 구현할 수 있습니다.\n",
    "    - ⭐ Prompt Engineering 실전 응용: 다양한 Prompt Engineering 기법(Zeroshot, Fewshot, Chain of Thought, Self-Consistency)을 활용하여 실제 시스템 프롬프트 구현.\n",
    "    - 실무형 챗봇 개발 역량: API와 LangChain을 결합하여 AI 기반 챗봇을 개발하며, 실무에서 요구되는 기술 역량을 기릅니다.\n",
    "\n",
    "4. 실습 핵심 내용\n",
    "    - 환경 설정: 실습에 필요한 환경을 구성합니다.\n",
    "        - 필수 라이브러리 설치\n",
    "        - API Key 발급 및 활용법 익히기\n",
    "    - Solar LLM 호출: Upstage의 Solar LLM을 호출하여 기본적인 작동 원리를 확인합니다.\n",
    "    - ⭐ Prompt Engineering : 다양한 Prompt Engineering 기법(Zeroshot, Fewshot, Chain of Thought, Self-Consistency)을 실제로 적용.\n",
    "    - LangChain 개념 이해: LangChain의 핵심 구조와 사용법을 학습합니다.\n",
    "    - LLM Chain 구현: LangChain을 사용하여 간단한 LLM Chain을 설계하고 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prerequisites**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "langchain == 0.3.12\n",
    "langchain-chroma == 0.1.4\n",
    "langchain-core == 0.3.25\n",
    "langchain-openai == 0.2.12\n",
    "langchain-text-splitters == 0.3.3\n",
    "langchain-upstage == 0.4.0\n",
    "getpass4 == 0.0.14.1\n",
    "openai == 1.57.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 라이브러리 소개\n",
    "  - `openai` : AI 모델과 상호작용할 수 있는 라이브러리\n",
    "  - `langchain` : 다양한 언어 모델 및 자연어 처리 기능들을 하나의 프레임워크에서 사용할 수 있도록 도와주는 라이브러리\n",
    "  -`langchain-upstage` : Upstage의 다양한 기술을 Langchain에서 활용할 수 있도록 지원하는 라이브러리\n",
    "  - `langchain-chroma` : Chroma 데이터베이스와 LangChain을 연동하여 데이터 검색과 처리를 더 효율적으로 수행할 수 있게 도와주는 라이브러리\n",
    "  - `getpass4` : 터미널에서 비밀번호나 중요한 정보를 보이지 않게 입력받을 수 있도록 하는 라이브러리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 약 1분 소요\n",
    "!pip install -qU openai langchain langchain-upstage langchain-chroma getpass4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise Overview**\n",
    "\n",
    "## **실습 목차**\n",
    "- **1. 환경 설정**: 실습에 필요한 환경을 미리 세팅해둡니다. <br>\n",
    "  - 1-1. Hello Solar!\n",
    "    - 1 발급받은 API Key 실습 파일에 적용하기\n",
    "    - 2 API Key를 사용하여 Upstage Solar Chat Model 활용해보기\n",
    "\n",
    "- **2. LLM Chain 이해하기** : LangChain을 사용하여 LLM Chain을 구현하는 방법을 배웁니다. <br>\n",
    "    - 2-1 LLM Chain\n",
    "    - 2-2 What is Langchain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 설정\n",
    "\n",
    "## 1-1. HELLO SOLAR!\n",
    "\n",
    "Upstage API Key를 발급받고 이를 통해 Solar-Mini-Chat Model을 호출해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1-1 UPSTAGE Credit 및 API Key 발급 받기\n",
    "\n",
    "1. 회원 가입 진행\n",
    "  1. <a href = \"https://console.upstage.ai/\">업스테이지 콘솔</a> 에 방문합니다.\n",
    "  2. 계정이 없다면, 구글 계정을 통해 회원가입을 진행합니다\n",
    "  3.  계정에 로그인 합니다.\n",
    "\n",
    "2. API Key 발급\n",
    "  1. <a href = \"https://console.upstage.ai/api-keys\">업스테이지 콘솔 - API Keys</a>페이지를 클릭합니다.\n",
    "  2. Create New key를 누르고, 발급받은 API key를 복사합니다.\n",
    "  3. 하단 셀을 실행한 후, 복사한 API Key를 넣습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title set API key\n",
    "import os\n",
    "import getpass  # 비밀번호를 보이지 않게 쓸 수 있는 라이브러리\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Get the Upstage API key using getpass\n",
    "if \"UPSTAGE_API_KEY\" not in os.environ or not os.environ[\"UPSTAGE_API_KEY\"]:\n",
    "    os.environ[\"UPSTAGE_API_KEY\"] = getpass.getpass(\"Enter your Upstage API key: \")\n",
    "\n",
    "print(\"API key has been set successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1-2 Solar Chat Model 사용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💻 코드 설명\n",
    "\n",
    "1. API Key와 기본 URL을 설정\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1tE21AG588xjXseoxt9iDxduEl8DmmuC0\" alt=\"Solar Model comparison\" width=\"800\"/>\n",
    "\n",
    "\n",
    "2. 채팅 요청을 생성\n",
    "   - 활용하고자 하는 **Chat 모델** 지정:  `\"solar-pro\"` or `\"solar-mini\"`\n",
    "   - Prompt(프롬프트) 지정\n",
    "      - Prompt란?\n",
    "        - LLM에게 Input 과 함께 제공하는 지시사항 혹은 예제들\n",
    "      - `System Prompt`\n",
    "        -  LLM에게 특정 역할 또는 작업(task)를 부여하여 응답을 생성할 수 있도록 설정할 수 있습니다.\n",
    "        -  대화가 포멀한지 캐주얼한 지, 친절한 어조 혹은 격식 있는 어조 등 대화 스타일을 설정할 수 있습니다.\n",
    "      - `User Prompt`\n",
    "        - 사용자가 LLM에게 보낼 쿼리/질문을 의미합니다.\n",
    "\n",
    "3. 모델의 응답 처리:\n",
    "   - `print()` :  전체 응답을 출력\n",
    "   - `response.choices[0].message.content` : 답변만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1-1-3 ⭐ 다양한 Prompt Engineering 방법론들 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"UPSTAGE_API_KEY\"], base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    "\n",
    "chat_result = client.chat.completions.create(\n",
    "    model=\"solar-pro\",  # 사용할 모델 입력\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"모든 답변은 존댓말로 답변해줘.\"},  # system prompt\n",
    "        {\"role\": \"user\", \"content\": \"한국의 수도는 어디야?\"},             # user prompt\n",
    "    ],\n",
    ")\n",
    "\n",
    "pprint(chat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(chat_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Few Shot Prompting**\n",
    "  - 모델이 더 자연스럽고 맥락에 맞는 응답을 생성할 수 있도록 몇 가지 예제를 제공하는 기법입니다. 이를 통해 모델이 주어진 입력에 어떻게 응답해야 하는지 학습할 수 있습니다.\n",
    "  - 위와 같이 아무 예시를 들지 않는 경우를 **Zero-Shot Prompting**이라고 하며, 아래는 예시를 여러개 제시한 **Few-Shot Prompting**의 예시입니다.\n",
    "  - `assistant` 라는 role을 지정하여 예시를 넣어줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# few shot prompts: examples or history\n",
    "chat_result = client.chat.completions.create(\n",
    "    model=\"solar-1-mini-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"모든 답변은 존댓말로 답변해줘.\"},\n",
    "\n",
    "        # few-shot prompting\n",
    "        {\"role\": \"user\", \"content\": \"프랑스의 수도는 어딘지 알아?\"},    # human query\n",
    "        {\"role\": \"assistant\", \"content\": \"알고 있습니다. 파리입니다.\"}, # LLM answer\n",
    "        {\"role\": \"user\", \"content\": \"일본의 수도는 어딘지 알아?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"알고 있습니다. 도쿄입니다.\"},\n",
    "\n",
    "        # user input\n",
    "        {\"role\": \"user\", \"content\": \"그렇다면 한국은?\"},\n",
    "    ],\n",
    ")\n",
    "pprint(chat_result)\n",
    "print(\"Message only:\")\n",
    "pprint(chat_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Chain of Thought Prompting**\n",
    "  - 모델이 복잡한 문제를 단계적으로 해결할 수 있도록 사고 과정을 명시적으로 제시하는 기법입니다. 이를 통해 모델이 단순한 답변을 생성하는 것이 아니라, 문제 해결 과정에서 필요한 논리적 추론 단계를 따를 수 있게 됩니다.\n",
    "  - 간단한 계산뿐 아니라 텍스트 분류, 논리적 사고, 의학적 진단 등 복잡한 문제에도 효과적으로 적용될 수 있습니다.\n",
    "  - 아래 코드에서 볼 수 있다시피, 중간의 사고 과정을 통해 답변을 하는 과정을 예제로 삽입함으로써 사용자의 질의에도 논리적인 사고 흐름으로 답변할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "chat_result = client.chat.completions.create(\n",
    "    model=\"solar-1-mini-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"모든 답변은 존댓말로 해주시고, 답변을 도출할 때는 chain-of-thought을 사용하십시오. 단, 최종 답변에서는 chain-of-thought을 직접 노출하지 말아주세요.\"},\n",
    "\n",
    "        # few-shot prompting: chain-of-thought을 보여주는 예시\n",
    "        # 예시 1\n",
    "        {\"role\": \"user\", \"content\": \"3과 5를 더하면 얼마일까요?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"\"아래는 제 사고 과정입니다.\n",
    "# chain-of-thought:\n",
    "# 1. 사용자가 3과 5를 더하라고 요청\n",
    "# 2. 간단한 덧셈: 3 + 5 = 8\n",
    "# 최종 답변은 8\n",
    "8\"\"\"},\n",
    "\n",
    "        # 예시 2\n",
    "        {\"role\": \"user\", \"content\": \"12에서 4를 뺀 값은 무엇일까요?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"\"아래는 제 사고 과정입니다.\n",
    "# chain-of-thought:\n",
    "# 1. 사용자가 12 - 4 계산 요청\n",
    "# 2. 12에서 4를 빼면 8\n",
    "# 최종 답변은 8\n",
    "8\"\"\"},\n",
    "\n",
    "        # user input - chain-of-thought을 포함하여 reasoning한 뒤 최종 답변만 출력\n",
    "        {\"role\": \"user\", \"content\": \"20을 4로 나누면 얼마인가요?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "pprint(chat_result)\n",
    "print(\"Message only:\")\n",
    "pprint(chat_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Self-Consistency Prompting**\n",
    "    - 모델이 복잡한 문제를 더 신뢰할 수 있는 방식으로 해결할 수 있도록, 여러 추론 경로를 탐색하고 가장 일관된 결과를 선택하는 기법입니다.\n",
    "    - 이를 통해 모델은 단순히 하나의 답을 생성하는 것이 아니라, 다양한 논리적 추론 경로에서 도출된 결과를 기반으로 최종 답변을 결정하게 됩니다.\n",
    "    - Self-Consistency는 특히 Chain of Thought(연쇄 추론) 기법과 결합되어 활용되며, 이를 통해 산술 계산, 논리적 사고, 텍스트 분류 등 복잡한 문제를 더 높은 정확도로 해결할 수 있습니다.\n",
    "    - 사용자는 각 추론 경로의 결과를 확인하거나, 자동으로 선택된 최종 결과만 받을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "chat_result = client.chat.completions.create(\n",
    "    model=\"solar-1-mini-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": (\n",
    "            \"모든 답변은 존댓말로 해주시고, 답변을 도출할 때는 Self-Consistency 방식의 chain-of-thought을 사용하십시오. \"\n",
    "            \"즉, 먼저 여러 개의 독립적인 사고 과정을 통해 답을 추론한 뒤, \"\n",
    "            \"가장 일관된 결론을 선택하여 최종 답변을 출력해주세요. \"\n",
    "            \"최종 답변에서는 chain-of-thought이나 여러 경로의 내용을 직접 노출하지 말아주세요.\"\n",
    "        )},\n",
    "\n",
    "        # 예시 1\n",
    "        {\"role\": \"user\", \"content\": \"10 더하기 2는 얼마인가요?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"\"아래는 여러 추론 경로입니다.\n",
    "# chain-of-thought (path A):\n",
    "# 1. 10 + 2 = 12\n",
    "# chain-of-thought (path B):\n",
    "# 1. 10을 2번 더하면 12\n",
    "# chain-of-thought (path C):\n",
    "# 1. 2를 10에 더하면 12\n",
    "# 모든 경로에서 12로 일관됨\n",
    "12\"\"\"},\n",
    "\n",
    "        # 예시 2\n",
    "        {\"role\": \"user\", \"content\": \"8 빼기 3은 얼마인가요?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"\"아래는 여러 추론 경로입니다.\n",
    "# chain-of-thought (path A):\n",
    "# 1. 8 - 3 = 5\n",
    "# chain-of-thought (path B):\n",
    "# 1. 3을 8에서 뺄 때, 5가 남음\n",
    "# chain-of-thought (path C):\n",
    "# 1. 8-3은 5\n",
    "# 모든 경로에서 5로 일관됨\n",
    "5\"\"\"},\n",
    "\n",
    "        # 실제 요청: 여러 개의 추론 경로를 만들고 가장 일관성 있는 답을 최종 출력\n",
    "        {\"role\": \"user\", \"content\": \"20을 4로 나누면 얼마인가요?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "pprint(chat_result)\n",
    "print(\"Message only:\")\n",
    "pprint(chat_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예시처럼 하나의 Agent 내에서 프롬프트로써 여러 판단을 내릴 수도 있으며, 아래 예시처럼 여러 Agent를 독립적으로 설정하여 예측을 수행하게 만들 수 있습니다.\n",
    "\n",
    "- 하나의 Agent로 수행할 경우, 그만큼 시간/비용 측면에서 유리하지만 하나의 LLM이 여러 의견을 동시에 생성해야 한다는 관점에서 성능 면에서는 떨어질 수 있습니다.\n",
    "- 여러 독립된 Agent로 구성할 경우, 성능 면에서는 우위를 점할 수 있으나 시간/비용 측면에서 trade-off가 존재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def call_llm(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"solar-1-mini-chat\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\n",
    "        \"당신은 질문에 답변할 때 논리적 사고 과정을 내부적으로 거치지만, \"\n",
    "        \"최종 답변에서는 그 과정을 노출하지 않고 존댓말로 결과만 제시합니다.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# 예: 단순한 수학 문제\n",
    "user_question = \"20을 4로 나누면 얼마인가요?\"\n",
    "\n",
    "# Agent A 호출\n",
    "agent_a_messages = [\n",
    "    system_message,\n",
    "    {\"role\": \"user\", \"content\": user_question}\n",
    "]\n",
    "agent_a_answer = call_llm(agent_a_messages)\n",
    "\n",
    "# Agent B 호출\n",
    "agent_b_messages = [\n",
    "    system_message,\n",
    "    {\"role\": \"user\", \"content\": user_question}\n",
    "]\n",
    "agent_b_answer = call_llm(agent_b_messages)\n",
    "\n",
    "# Agent C 호출\n",
    "agent_c_messages = [\n",
    "    system_message,\n",
    "    {\"role\": \"user\", \"content\": user_question}\n",
    "]\n",
    "agent_c_answer = call_llm(agent_c_messages)\n",
    "\n",
    "# 이제 Aggregator Agent에게 세 개의 결과를 전달.\n",
    "# Aggregator Agent는 Self-Consistency 전략:\n",
    "# 1) 내부적으로 세 답변을 보고 reasoning (Chain-of-thought)\n",
    "# 2) 가장 일관성 있고 합리적인 결론을 도출\n",
    "# 3) 최종적으로 reasoning 없이 결과만 답변\n",
    "aggregator_system = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\n",
    "        \"당신은 Aggregator Agent입니다. 세 명의 다른 에이전트(A, B, C)가 각각 동일한 질문에 대해 답변을 제시했습니다. \"\n",
    "        \"당신은 이들의 답변을 보고 일관성 있는 최종 결론을 도출하십시오. \"\n",
    "        \"사고 과정은 내부적으로만 진행하고, 최종 답변에서는 절대 노출하지 말며, \"\n",
    "        \"존댓말로 명료한 결과만 최종적으로 제시하세요.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "aggregator_user = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": (\n",
    "        f\"아래는 세 에이전트의 답변입니다.\\n\\n\"\n",
    "        f\"Agent A: {agent_a_answer}\\n\"\n",
    "        f\"Agent B: {agent_b_answer}\\n\"\n",
    "        f\"Agent C: {agent_c_answer}\\n\\n\"\n",
    "        \"이들의 답변을 종합하여 최종적으로 가장 일관된 답변을 제시해주세요.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "aggregator_result = call_llm([aggregator_system, aggregator_user])\n",
    "\n",
    "pprint(aggregator_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**더 다양한 Prompt Engineering 공부해보기**\n",
    "\n",
    "1. [[Prompt Engineering - Part 1] Maximizing the Use of LLM with Prompt Design](https://www.upstage.ai/feed/insight/prompt-engineering-guide-maximizing-the-use-of-llm-with-prompt-design)\n",
    "2. [[Prompt Engineering - Part 2] The Essence of Prompt Engineering: A Comprehensive Guide to Maximizing LLM Usage](https://www.upstage.ai/feed/insight/prompt-engineering-guide-to-maximizing-llm-usage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LLM Chain과 Langchain 이해하기\n",
    "\n",
    "- 2-1 LLM Chain\n",
    "  - LLM Chain이 무엇인지 이해합니다.\n",
    "- 2-2 What is Langchain?\n",
    "  - LLM Chain을 제공하는 Langchain 라이브러리에 대한 설명을 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Recap.) LLM의 구성요소\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1UUy0ulWxBe5BapXufz6oT5cR9_As8Qta\" alt=\"Definition of DP\" width=\"600\" />\n",
    "\n",
    "1. `User Input` : 사용자가 LLM에게 보낼 질문/쿼리/입력\n",
    "2. `Prompt`\n",
    "  - Task Description : LLM에게 특정 역할/task를 부여함. (e.g. 질의응답, 요약, 번역 등)\n",
    "  - Output Indicator : 대화 스타일, 어조 등을 부여하는 등 output 형식들을 지정\n",
    "  - Example Inputs : Few-Shot Prompting, LLM이 맥락을 더 파악할 수 있도록 예제를 넣어줌\n",
    "3. `LLM Model` : user input을 바탕으로 output을 생성할 대규모 텍스트 데이터로 사전학습된 AI 모델\n",
    "4. `Output` : LLM이 생성해내는 Text 답변/응답\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **위 구성요소들 (User Input, Prompt, LLM Model)을 손쉽게 연결해서 쉽게 output 답변을 생성할 수 있는 도구가 \"LLM Chain\"**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2-1 LLM Chain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 📘 LLM Chain이란?\n",
    "\n",
    " LLM Chain(체인)이란, **여러 구성 요소들을 결합**하여 **LLM의 출력을 생성하는 프로세스**를 의미합니다. 이는 입력 프롬프트를 모델에 전달하고, 모델의 출력을 받아 원하는 형식으로 처리할 수 있도록 구성된 파이프라인입니다. 각 단계별로 어떻게 Chain을 구성하는 지 알아봅시다.\n",
    "\n",
    "#### LLM Chain 구성 요소\n",
    "- **LLM(Large Language Model)**  : 모델 정의\n",
    "- **Prompt(프롬프트)** : LLM에게 쿼리(질문)과 함께 입력하는 예제와 지시사항  \n",
    "- Output Parser(출력 파서, *optional*) : 출력 결과의 형태를 지정할 수 있는 도구\n",
    "\n",
    "\n",
    "### langchain_upstage\n",
    "\n",
    "- langchain을 사용할 수 있는 여러 Provider들(e.g. upstage, huggingface, openai, google 등)이 직접 제공하는 Langchain의 통합 모듈.\n",
    "\n",
    "> https://python.langchain.com/docs/integrations/providers/upstage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💻 코드 설명\n",
    "\n",
    "1. 선호하는 LLM 정의:\n",
    "   - `langchain_upstage`에서 `ChatUpstage` 클래스를 가져옵니다.\n",
    "   - `ChatUpstage` 인스턴스를 생성하고 변수 `llm`에 할당합니다.\n",
    "\n",
    "2. 입력 프롬프트 정의:\n",
    "   - `langchain_core.prompts`에서 `ChatPromptTemplate` 클래스를 가져옵니다.\n",
    "   - `from_messages()` 메서드를 사용해 `ChatPromptTemplate` 인스턴스를 생성합니다.\n",
    "   -  system prompt, few-shot prompting을 위한 예제들, user input 등을 포함한 메시지 리스트를 제공합니다.\n",
    "\n",
    "3. 출력 파서 정의 :\n",
    "   - `langchain_core.output_parsers`에서 `StrOutputParser` 클래스를 가져옵니다.\n",
    "   - 더 자세한 Parser에 대한 내용은 [Langchain Guide](https://python.langchain.com/docs/concepts/output_parsers/) 참고 바랍니다.\n",
    "\n",
    "4. 체인 정의 :\n",
    "   - `rag_with_history_prompt`, `llm`, `StrOutputParser()`를 파이프(`|`) 연산자를 사용하여 결합해 체인을 만듭니다.\n",
    "\n",
    "4. 체인 호출:\n",
    "   - `chain` 객체의 `invoke()` 메서드를 호출하고 빈 딕셔너리(`{}`)를 입력값으로 전달합니다.\n",
    "   - 체인에서 얻은 응답을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2-2 What is Langchain?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.licdn.com/dms/image/D4D12AQGQQFHNeQJRgQ/article-cover_image-shrink_720_1280/0/1711873462713?e=2147483647&v=beta&t=u5ls9p4LHatE_PxtiNIm23lIFGMaAjp-XHdV7TwwDxE\" alt=\"Langchain\" width=\"300\" />\n",
    "\n",
    "- LLM Chain을 제공해주는 라이브러리!\n",
    "\n",
    "- LangChain은 대규모 언어 모델(LLM)을 활용해 AI Application(e.g.챗봇, QA 시스템 등)을 쉽게 개발할 수 있도록 도와주는 프레임워크입니다. 이는 누구나 LLM을 이용해 쉽고 빠르게 애플리케이션을 만들고 배포할 수 있도록 지원합니다. 프로그래밍 경험이 없어도 LangChain의 다양한 도구와 템플릿을 통해 필요한 AI 애플리케이션을 만들 수 있습니다. 복잡한 개발 과정을 간소화하여 AI 애플리케이션 개발을 더 쉽게 접근할 수 있게 해줍니다.\n",
    "\n",
    "\n",
    "#### Langchain의 구성요소\n",
    "1. [Langchain Library](https://python.langchain.com/v0.2/docs/introduction/) : Lanchain의 다양한 기능을 사용할 수 있게 구현해둔 패키지\n",
    "   - **langchain-core**: LangChain의 가장 기본 문법\n",
    "   - **Integrated Package** : 다른 AI 도구와 LangChain을 쉽게 연결할 수 있는 패키지 (e.g. [langchain-upstage](https://python.langchain.com/docs/integrations/providers/upstage/), [langchain_chroma](https://python.langchain.com/docs/integrations/providers/chroma/))\n",
    "   - **langchain**: 애플리케이션 구성에 필요한 체인, 에이전트, 정보 검색 전략 등을 제공하여 애플리케이션의 '두뇌' 역할을 합니다.\n",
    "\n",
    "2. [Langchain Templates](https://templates.langchain.com/): 다양한 작업에 맞춘 템플릿으로, 개발자들이 애플리케이션을 더 빠르게 설정하고 실행할 수 있도록 돕습니다.\n",
    "3. [LangServe](https://python.langchain.com/docs/langserve/): LangChain으로 만든 애플리케이션을 REST API로 쉽게 배포할 수 있게 해주는 도구입니다.\n",
    "4. [LangSmith](https://docs.smith.langchain.com/): 개발자가 애플리케이션을 디버그하고 테스트하며 모니터링할 수 있도록 도와주는 플랫폼입니다.\n",
    "5. [LangGraph](https://www.langchain.com/langgraph): LLM이 가지는 여러 상태들을 관리하여 Agent를 구조화하여 구현할 수 있는 프레임워크입니다.\n",
    "\n",
    "더 자세한 Langchain에 대한 설명은 <a href = \"https://python.langchain.com/docs/introduction/\">Langchain Documentation</a> 을 확인하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# LLM Chain 구성하는 법.\n",
    "# 1. llm 정의, 2. prompt 정의, 3. chain 정의, 4. chain 호출\n",
    "\n",
    "# 1. define your favorate llm, solar\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "# 2. define chat prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate # '대화' 형태로 prompt template 생성\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"모든 답변은 존댓말로 답변해줘.\"),\n",
    "\n",
    "        # few-shot prompting\n",
    "        (\"human\", \"프랑스의 수도는 어딘지 알아?\"),  # human request\n",
    "        (\"ai\", \"알고 있습니다. 파리입니다.\"),      # LLM response\n",
    "        (\"human\", \"일본의 수도는 어딘지 알아?\"),\n",
    "        (\"ai\",  \"알고 있습니다. 도쿄입니다.\"),\n",
    "\n",
    "        # User Query\n",
    "        (\"human\", \"그렇다면, 한국은?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. define chain\n",
    "from langchain_core.output_parsers import StrOutputParser #문자열(text, string)만 나오게 하는 출력 파서\n",
    "\n",
    "# chain = prompt | llm  # without output parser\n",
    "\n",
    "chain = prompt | llm | StrOutputParser() # with output parser\n",
    "\n",
    "# 4. invoke the chain\n",
    "c_result = chain.invoke({})\n",
    "print(c_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 실습 마무리\n",
    "\n",
    "이번 실습에서는 Upstage Solar LLM과 LangChain을 활용하여 AI 기반 ChatBot을 단계별로 구현하는 방법을 학습하였습니다. 실습을 통해 다음과 같은 주요 기술과 역량을 익혔습니다:\n",
    "\n",
    "- LangChain의 개념 이해: LangChain의 구조와 활용법을 파악하였으며, 이를 기반으로 LLM Chain을 설계하고 실행하는 방법을 익혔습니다.\n",
    "- Prompt Engineering 적용: 다양한 Prompt Engineering 기법(Zeroshot, Fewshot, Chain of Thought, Self-Consistency)을 적용하여 모델의 성능을 효과적으로 활용하는 방법을 배웠습니다.\n",
    "- Solar LLM 활용: Solar API 호출을 통해 실질적인 챗봇 개발에 필요한 기술을 습득하였습니다.\n",
    "\n",
    "이번 실습을 통해 AI 기반 언어 모델과 LangChain의 실무 활용 가능성을 확인하며, 챗봇 개발의 기초를 다질 수 있었습니다. 다음 실습에서는 RAG를 붙여서 구현하는 방법에 대해 알아보도록 하겠습니다.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
